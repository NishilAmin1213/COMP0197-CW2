{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "bxwfi5dnYIPO",
        "akGvrO70YoRv",
        "6v2KPLYqZBdM",
        "JVKQ33-LZRcp",
        "aTWndJHgZyEJ",
        "XK4IExm1aFU9"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.5.0 torchvision --index-url https://download.pytorch.org/whl/cpu"
      ],
      "metadata": {
        "id": "ycmnUrgELVvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setting Up"
      ],
      "metadata": {
        "id": "s5drQ8WNM9oV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.utils.data import DataLoader, random_split, Dataset\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import torchvision.transforms.v2 as T\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "# For bounding box manipulations (if needed)\n",
        "from torchvision.ops import box_convert, box_iou\n",
        "\n",
        "# If using Albumentations for data augmentations\n",
        "# pip install albumentations\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# If you want to use a CRF post-processing library or morphological ops\n",
        "# pip install pydensecrf, opencv-python, etc.\n",
        "# import pydensecrf.densecrf as dcrf\n",
        "\n",
        "# Import the data loading procedure you have written.\n",
        "# from dataset_load import train_dataset, val_dataset, train_loader, val_loader\n",
        "# OR define your train_loader and val_loader below similarly."
      ],
      "metadata": {
        "id": "C3pb3fmZMbP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "Ig2FcMQ7LYQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Load the Oxford-IIIT Pet Dataset (Classification Labels)\n",
        "\n",
        "We only need image–class pairs, so we use:"
      ],
      "metadata": {
        "id": "ft1KI4ghLqlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import OxfordIIITPet\n",
        "\n",
        "# Define transforms\n",
        "train_transform = T.Compose([\n",
        "    T.Resize((256, 256)),\n",
        "    T.RandomResizedCrop((224, 224), scale=(0.8, 1.0)),\n",
        "    T.RandomHorizontalFlip(p=0.5),\n",
        "    T.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "val_transform = T.Compose([\n",
        "    T.Resize((224, 224)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Custom wrapper to apply transform on-the-fly\n",
        "class TransformDataset(Dataset):\n",
        "    def __init__(self, subset, transform):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, target = self.subset[idx]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, target\n",
        "\n",
        "# Load dataset without transform first\n",
        "base_dataset = OxfordIIITPet(\n",
        "    root=\"./oxford_iiit_data\",\n",
        "    download=True,\n",
        "    target_types=\"category\",  # classification labels only\n",
        "    split=\"trainval\",\n",
        "    transform=None,\n",
        ")\n",
        "\n",
        "# Train/val split\n",
        "train_size = int(0.85 * len(base_dataset))\n",
        "val_size = len(base_dataset) - train_size\n",
        "train_subset, val_subset = random_split(base_dataset, [train_size, val_size])\n",
        "\n",
        "# Wrap subsets with respective transforms\n",
        "train_ds = TransformDataset(train_subset, train_transform)\n",
        "val_ds = TransformDataset(val_subset, val_transform)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "num_classes = 37  # There are 37 pet breeds"
      ],
      "metadata": {
        "id": "H-uxtrBBLlQq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d36c1948-10d7-46f9-bf58-af8332e290dd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 792M/792M [00:37<00:00, 20.9MB/s]\n",
            "100%|██████████| 19.2M/19.2M [00:02<00:00, 8.74MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model Initialization (Classifier)\n",
        "\n",
        "Pre-trained ResNet50 for classification."
      ],
      "metadata": {
        "id": "uxOuG_VSSjB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_resnet50_classifier_model(num_classes=37):\n",
        "    \"\"\"\n",
        "    Oxford-IIIT has 37 categories (pet breeds) for classification.\n",
        "    \"\"\"\n",
        "    model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
        "\n",
        "    # Replace the final FC layer\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, num_classes)\n",
        "    return model"
      ],
      "metadata": {
        "id": "KoHECEcmSldX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Training the Classifier\n",
        "\n",
        "We’ll include:\n",
        "  *\tLearning rate scheduling (StepLR as an example)\n",
        "  *\tWeight decay\n",
        "  *\tGradient clipping"
      ],
      "metadata": {
        "id": "qF6uKHWnU4ii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classifier(model, train_loader, val_loader,\n",
        "                     num_epochs=10, lr=1e-3,\n",
        "                     weight_decay=1e-4, clip_grad_norm=None, save_path=\"classifier.pth\"):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = StepLR(optimizer, step_size=5, gamma=0.1)  # example scheduler\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for images, labels in train_loader:  # bboxes not loaded into train loader so not used in training here\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            if clip_grad_norm is not None:\n",
        "              torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_grad_norm)\n",
        "\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        val_acc = evaluate_classifier(model, val_loader, device)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] | Loss: {epoch_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "\n",
        "    # SAVE the classifier model after training\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"Classifier model saved to {save_path}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def evaluate_classifier(model, val_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "md57ksYHU4QD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Generating Raw CAMs (Grad-CAM)\n",
        "\n",
        "A straightforward Grad-CAM approach:"
      ],
      "metadata": {
        "id": "bxwfi5dnYIPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GradCAM:\n",
        "    \"\"\"\n",
        "    Simple Grad-CAM for ResNet-based networks.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, target_layer_name=\"layer4\"):\n",
        "        self.model = model\n",
        "        self.model.eval()\n",
        "\n",
        "        # Hook the target layer\n",
        "        self.target_layer = None\n",
        "        for name, module in self.model.named_children():\n",
        "            if name == target_layer_name:\n",
        "                self.target_layer = module\n",
        "                break\n",
        "\n",
        "        if self.target_layer is None:\n",
        "            raise ValueError(f\"Layer {target_layer_name} not found in model\")\n",
        "\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations = output\n",
        "\n",
        "        def backward_hook(module, grad_in, grad_out):\n",
        "            self.gradients = grad_out[0]\n",
        "\n",
        "        self.target_layer.register_forward_hook(forward_hook)\n",
        "        self.target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "    def __call__(self, x, class_idx=None):\n",
        "        \"\"\"\n",
        "        x: input image tensor of shape [B, C, H, W]\n",
        "        class_idx: which class index to compute CAM for. If None, uses argmax\n",
        "        Returns: CAM for each image in the batch\n",
        "        \"\"\"\n",
        "        logits = self.model(x)  # forward pass\n",
        "        if class_idx is None:\n",
        "            class_idx = torch.argmax(logits, dim=1)\n",
        "\n",
        "        # Compute gradients w.r.t. target class\n",
        "        one_hot = torch.zeros_like(logits)\n",
        "        for i in range(logits.size(0)):\n",
        "            one_hot[i, class_idx[i]] = 1.0\n",
        "\n",
        "        self.model.zero_grad()\n",
        "        logits.backward(gradient=one_hot, retain_graph=True)\n",
        "\n",
        "        gradients = self.gradients  # [B, C, H', W']\n",
        "        activations = self.activations  # [B, C, H', W']\n",
        "\n",
        "        # Global-average-pool the gradients\n",
        "        weights = torch.mean(gradients, dim=(2, 3), keepdim=True)  # [B, C, 1, 1]\n",
        "\n",
        "        # Weighted sum of activations\n",
        "        cams = (weights * activations).sum(dim=1, keepdim=True)  # [B, 1, H', W']\n",
        "\n",
        "        # ReLU\n",
        "        cams = F.relu(cams)\n",
        "\n",
        "        # Normalize each CAM to [0,1]\n",
        "        cams = cams - cams.view(cams.size(0), -1).min(dim=1)[0].view(cams.size(0),1,1,1)\n",
        "        cams = cams / (cams.view(cams.size(0), -1).max(dim=1)[0].view(cams.size(0),1,1,1) + 1e-8)\n",
        "\n",
        "        return cams"
      ],
      "metadata": {
        "id": "GF0Nt5awYMJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bounding Box Usage:**\n",
        "*\tIf you have bounding boxes for each image, you can mask out or scale down CAMs outside the bounding box. This can reduce background confusion. Below we do a simple approach: zero out CAM values outside the bounding box."
      ],
      "metadata": {
        "id": "sPQm8-2IYPve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_bbox_to_cam(cam, bbox, H, W):\n",
        "    \"\"\"\n",
        "    cam: [H', W'] (already 2D)\n",
        "    bbox: [x_min, y_min, x_max, y_max] in the original image scale\n",
        "    H, W: original input image size\n",
        "    We assume the cam is upsampled to HxW or we do the reverse scale if needed\n",
        "    For simplicity, let's assume we already have cam upsampled to the input size [H, W].\n",
        "    \"\"\"\n",
        "    # Ensure we clamp the bbox within [0, W]x[0, H]\n",
        "    x_min, y_min, x_max, y_max = map(int, bbox)\n",
        "    x_min = max(x_min, 0); y_min = max(y_min, 0)\n",
        "    x_max = min(x_max, W-1); y_max = min(y_max, H-1)\n",
        "\n",
        "    # Zero out outside the bounding box\n",
        "    mask = np.zeros((H, W), dtype=np.float32)\n",
        "    mask[y_min:y_max, x_min:x_max] = 1.0\n",
        "    cam_bbox = cam * mask\n",
        "    return cam_bbox\n",
        "\n",
        "def generate_cams(model, data_loader, gradcam, device,\n",
        "                  apply_bbox=True, output_dir=\"cams_out\"):\n",
        "    \"\"\"\n",
        "    Generate and save CAMs.\n",
        "    We'll upsample the CAM to the original image size,\n",
        "    then optionally mask it with the bounding box.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    model.eval()\n",
        "    gradcam.model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (images, labels, bboxes) in enumerate(data_loader):\n",
        "            images = images.to(device)\n",
        "\n",
        "            # GradCAM call needs a forward and backward pass,\n",
        "            # so let's do it with requires_grad\n",
        "            with torch.enable_grad():\n",
        "                cams_batch = gradcam(images, class_idx=labels.to(device))\n",
        "\n",
        "            # cams_batch shape: [B, 1, H', W']\n",
        "            # Upsample CAM to match input size (e.g. 224x224 if that's your input transform)\n",
        "            # using bilinear interpolation\n",
        "            upsampled_cams = F.interpolate(cams_batch, size=(images.shape[2], images.shape[3]),\n",
        "                                           mode='bilinear', align_corners=False)\n",
        "\n",
        "            upsampled_cams = upsampled_cams.squeeze(1).cpu().numpy()  # shape [B, H, W]\n",
        "\n",
        "            # If original images are bigger than 224, you may have to re-scale bounding boxes and\n",
        "            # possibly re-scale the CAM again. This depends on your transformations.\n",
        "\n",
        "            for b in range(upsampled_cams.shape[0]):\n",
        "                cam_2d = upsampled_cams[b]\n",
        "                if apply_bbox:\n",
        "                    # bboxes[b] is the bounding box for this image in original scale\n",
        "                    # Make sure the scale is consistent with your input size!\n",
        "                    H, W = cam_2d.shape\n",
        "                    cam_2d = apply_bbox_to_cam(cam_2d, bboxes[b], H, W)\n",
        "\n",
        "                # Save or return the CAM\n",
        "                cam_path = os.path.join(output_dir, f\"cam_{i*data_loader.batch_size + b}.npy\")\n",
        "                np.save(cam_path, cam_2d)\n",
        "\n",
        "    print(\"CAM generation complete!\")"
      ],
      "metadata": {
        "id": "IVFw4lnRYeUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Apply ReCAM (Refinement / Expansion)\n",
        "\n",
        "Below is an illustrative approach inspired by the ReCAM paper. This usually involves:\n",
        "  *\tRe-scoring the CAM to ensure more complete coverage of the object.\n",
        "  *\tPossibly iterative expansions (e.g. random erasing, multi-scale expansions).\n",
        "  *\tWe show a simplified version that scales up smaller areas, and we add expansions."
      ],
      "metadata": {
        "id": "akGvrO70YoRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recam_refinement(cam, expansion_factor=1.2, threshold=0.3):\n",
        "    \"\"\"\n",
        "    Simplified approach:\n",
        "    1. If the average of the top region is below a certain threshold,\n",
        "       push it up (expand coverage).\n",
        "    2. You could also do iterative random erasing or multi-scale expansions.\n",
        "    \"\"\"\n",
        "    # cam: 2D np array [H, W] in [0,1]\n",
        "    # Step 1: thresholding\n",
        "    mask = (cam >= threshold).astype(np.uint8)\n",
        "    coverage = mask.sum() / (cam.shape[0]*cam.shape[1])\n",
        "\n",
        "    # If coverage < some ratio, inflate the activation\n",
        "    if coverage < 0.1:\n",
        "        cam = cam * expansion_factor\n",
        "        cam = np.clip(cam, 0, 1)\n",
        "\n",
        "    # Re-threshold\n",
        "    return cam\n",
        "\n",
        "def refine_cams_with_recam(cam_dir, refined_dir=\"cams_refined\",\n",
        "                           threshold=0.3, expansion_factor=1.2):\n",
        "    os.makedirs(refined_dir, exist_ok=True)\n",
        "\n",
        "    cam_files = [f for f in os.listdir(cam_dir) if f.endswith('.npy')]\n",
        "    for cfile in cam_files:\n",
        "        cam_path = os.path.join(cam_dir, cfile)\n",
        "        cam = np.load(cam_path)\n",
        "\n",
        "        refined_cam = recam_refinement(cam, expansion_factor, threshold)\n",
        "\n",
        "        # Save\n",
        "        refined_path = os.path.join(refined_dir, cfile)\n",
        "        np.save(refined_path, refined_cam)\n",
        "\n",
        "    print(\"ReCAM refinement complete!\")"
      ],
      "metadata": {
        "id": "izWbRKYZY8bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Pseudo-Label Filtering\n",
        "\n",
        "A simple approach:\n",
        "\t•\tWe binarize or do top-k%.\n",
        "\t•\tFor example, if we do a threshold t=0.5, anything above 0.5 = foreground, else background."
      ],
      "metadata": {
        "id": "6v2KPLYqZBdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_pseudo_masks(refined_cam_dir, output_mask_dir=\"pseudo_masks\", threshold=0.5):\n",
        "    os.makedirs(output_mask_dir, exist_ok=True)\n",
        "\n",
        "    cam_files = [f for f in os.listdir(refined_cam_dir) if f.endswith('.npy')]\n",
        "\n",
        "    for cfile in cam_files:\n",
        "        cam_path = os.path.join(refined_cam_dir, cfile)\n",
        "        cam = np.load(cam_path)\n",
        "\n",
        "        # Binarize\n",
        "        pseudo_mask = (cam >= threshold).astype(np.uint8)\n",
        "\n",
        "        # Save as PNG, for instance\n",
        "        mask_path = os.path.join(output_mask_dir, cfile.replace('.npy', '.png'))\n",
        "        cv2.imwrite(mask_path, pseudo_mask*255)\n",
        "\n",
        "    print(\"Pseudo-label generation complete!\")"
      ],
      "metadata": {
        "id": "id8ylnbEZIXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Train Segmentation Model (DeepLab V3+)\n",
        "Now we use the generated pseudo masks as “ground truth” for training. We’ll assume you have a new dataset that loads:\n",
        "  *\t(Image, PseudoMask)\n",
        "  *\tPossibly ignoring bounding boxes at this stage."
      ],
      "metadata": {
        "id": "JVKQ33-LZRcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PseudoSegDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_paths, mask_paths, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.mask_paths = mask_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        mask_path = self.mask_paths[idx]\n",
        "\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # 0/255\n",
        "        mask = (mask > 127).astype(np.uint8)  # binarize as 0/1\n",
        "\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image = augmented[\"image\"]\n",
        "            mask = augmented[\"mask\"]\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "def get_deeplab_v3(num_classes=2, pretrained=True):\n",
        "    model = deeplabv3_resnet50(pretrained=pretrained)\n",
        "    model.classifier[4] = nn.Conv2d(256, num_classes, kernel_size=1)  # output channels = 2 (background, foreground)\n",
        "    return model"
      ],
      "metadata": {
        "id": "Cei629m1Zdyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training loop for segmentation:** (Need to save weights)"
      ],
      "metadata": {
        "id": "G4yUqnCnZjic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_segmentation_model(seg_model, seg_train_loader, seg_val_loader,\n",
        "                            num_epochs=10, lr=1e-3, weight_decay=1e-4):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    seg_model = seg_model.to(device)\n",
        "\n",
        "    optimizer = optim.Adam(seg_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "    # For binary segmentation, we can use BCEWithLogitsLoss:\n",
        "    criterion = nn.CrossEntropyLoss()  # if 2-class with integer mask[0 or 1]\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        seg_model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for images, masks in seg_train_loader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.long().to(device)  # ensure correct type for CE\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = seg_model(images)['out']  # DeepLab outputs a dict\n",
        "            loss = criterion(outputs, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "        train_loss = total_loss / len(seg_train_loader.dataset)\n",
        "        val_loss = evaluate_segmentation(seg_model, seg_val_loader, criterion, device)\n",
        "\n",
        "        print(f\"[{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    return seg_model\n",
        "\n",
        "def evaluate_segmentation(seg_model, seg_val_loader, criterion, device):\n",
        "    seg_model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for images, masks in seg_val_loader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.long().to(device)\n",
        "            outputs = seg_model(images)['out']\n",
        "            loss = criterion(outputs, masks)\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "\n",
        "    return total_loss / len(seg_val_loader.dataset)"
      ],
      "metadata": {
        "id": "sdbBqcyXZjO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Post-Processing (Optional: CRF / Morphological Ops)\n",
        "\n",
        "You can apply a dense CRF or morphological opening/closing to each predicted segmentation mask. Here is a simple morphological example:"
      ],
      "metadata": {
        "id": "aTWndJHgZyEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def morphological_refinement(pred_mask, kernel_size=3):\n",
        "    \"\"\"\n",
        "    pred_mask: np.array of shape [H, W], values in {0,1}.\n",
        "    \"\"\"\n",
        "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
        "    # morphological closing -> remove small holes\n",
        "    refined = cv2.morphologyEx(pred_mask, cv2.MORPH_CLOSE, kernel)\n",
        "    return refined"
      ],
      "metadata": {
        "id": "etW-T15cZ2tR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CRF usage typically looks like:**"
      ],
      "metadata": {
        "id": "gP7eTuJkZ7wY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Outline (not a fully functional snippet):\n",
        "# def apply_crf(image, pred_mask_prob):\n",
        "#     # Setup the dense CRF using pydensecrf\n",
        "#     d = dcrf.DenseCRF2D(W, H, 2)  # 2 classes\n",
        "#     # set unary potentials from the pred_mask_prob\n",
        "#     # set pairwise potentials\n",
        "#     # run inference\n",
        "#     # return refined mask\n",
        "#     pass"
      ],
      "metadata": {
        "id": "Bz4l_TtYZ9dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Evaluation on Segmentation Metrics\n",
        "\n",
        "To evaluate, we typically compute mIoU or Dice on the ground-truth masks. For Oxford-IIIT, you have ground-truth segmentation masks. We can do something like:"
      ],
      "metadata": {
        "id": "XK4IExm1aFU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_mIoU(pred_mask, gt_mask, num_classes=2):\n",
        "    \"\"\"\n",
        "    pred_mask, gt_mask: [H, W] in {0,1} or {0,...,num_classes-1}\n",
        "    For binary, it's 0 or 1.\n",
        "    \"\"\"\n",
        "    # Flatten\n",
        "    pred_flat = pred_mask.flatten()\n",
        "    gt_flat = gt_mask.flatten()\n",
        "\n",
        "    ious = []\n",
        "    for c in range(num_classes):\n",
        "        pred_inds = (pred_flat == c)\n",
        "        gt_inds = (gt_flat == c)\n",
        "        intersection = (pred_inds & gt_inds).sum()\n",
        "        union = (pred_inds | gt_inds).sum()\n",
        "        if union == 0:\n",
        "            iou = 1 if intersection == 0 else 0\n",
        "        else:\n",
        "            iou = intersection / union\n",
        "        ious.append(iou)\n",
        "    return np.mean(ious)\n",
        "\n",
        "def evaluate_on_test(seg_model, test_loader, device):\n",
        "    seg_model.eval()\n",
        "    all_ious = []\n",
        "    with torch.no_grad():\n",
        "        for images, masks in test_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = seg_model(images)['out']\n",
        "            # get predictions\n",
        "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "            # compare with ground truth\n",
        "            for b in range(preds.shape[0]):\n",
        "                pred_mask = preds[b]\n",
        "                gt_mask = masks[b].numpy()\n",
        "                iou = compute_mIoU(pred_mask, gt_mask, num_classes=2)\n",
        "                all_ious.append(iou)\n",
        "    return np.mean(all_ious)"
      ],
      "metadata": {
        "id": "hc6kB7oVaP4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "LSDgLVM7T1PB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # 1. Load your train/val data\n",
        "    # from dataset_load import train_loader, val_loader  # custom\n",
        "    # train_loader, val_loader = ...\n",
        "\n",
        "    # 2. Create a classifier, train it\n",
        "    classifier = get_resnet50_classifier_model(num_classes=37)\n",
        "    classifier = train_classifier(\n",
        "        model=classifier,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        num_epochs=10, lr=1e-3, weight_decay=1e-4, clip_grad_norm=None, save_path=\"classifier.pth\"\n",
        "    ) # If gradient clipping is needed we should assign a optimal clip_grad_norm value Ex: 5.0\n",
        "\n",
        "    ## Load trainval dataset with Bounding Boxes\n",
        "\n",
        "    # 3. Generate raw CAMs\n",
        "    # gradcam = GradCAM(classifier, target_layer_name=\"layer4\")  # for ResNet\n",
        "    # generate_cams(\n",
        "    #     model=classifier,\n",
        "    #     data_loader=train_loader,   # or a combined trainval loader if you want CAM for all\n",
        "    #     gradcam=gradcam,\n",
        "    #     device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    #     apply_bbox=True,\n",
        "    #     output_dir=\"cams_out\"\n",
        "    # )\n",
        "\n",
        "    # # 4. Apply ReCAM refinement\n",
        "    # refine_cams_with_recam(\"cams_out\", refined_dir=\"cams_refined\",\n",
        "    #                        threshold=0.3, expansion_factor=1.2)\n",
        "\n",
        "    # # 5. Generate pseudo masks\n",
        "    # generate_pseudo_masks(refined_cam_dir=\"cams_refined\",\n",
        "    #                       output_mask_dir=\"pseudo_masks\",\n",
        "    #                       threshold=0.5)\n",
        "\n",
        "    # # 6. Train a segmentation model (DeepLab) with these pseudo masks\n",
        "    # # Build a dataset from your images and \"pseudo_masks\"\n",
        "    # images_list = ...  # list of paths to the original images\n",
        "    # pseudo_masks_list = ...  # matching list of paths in \"pseudo_masks\"\n",
        "\n",
        "    # Maybe We can use different transforms here for images and pseudo_masks for segmentation\n",
        "    # instead of using the transforms used for classification\n",
        "\n",
        "    # seg_dataset = PseudoSegDataset(images_list, pseudo_masks_list, transform=train_transform)\n",
        "    # seg_train_loader = DataLoader(seg_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "    # # If you have a separate validation set with pseudo masks, define seg_val_loader similarly\n",
        "    # seg_val_loader = ...\n",
        "\n",
        "    # seg_model = get_deeplab_v3(num_classes=2, pretrained=True)\n",
        "    # seg_model = train_segmentation_model(\n",
        "    #     seg_model, seg_train_loader, seg_val_loader,\n",
        "    #     num_epochs=10, lr=1e-3, weight_decay=1e-4\n",
        "    # )\n",
        "\n",
        "    # 7. Optional: Post-processing (CRF/morphological ops) inside your inference loop.\n",
        "\n",
        "    # 8. Testing / Final evaluation\n",
        "    # test_loader -> uses ground-truth masks\n",
        "    # test_miou = evaluate_on_test(seg_model, test_loader, torch.device(\"cuda\"))\n",
        "    # print(f\"Test mIoU: {test_miou}\")\n",
        "\n",
        "    print(\"Workflow complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "_p87DUHhT2qb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd5ec927-1be2-47a1-d15a-77933df3b021"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] | Loss: 1.5643 | Val Acc: 0.5326\n",
            "Epoch [2/10] | Loss: 0.8846 | Val Acc: 0.5236\n",
            "Epoch [3/10] | Loss: 0.6401 | Val Acc: 0.7391\n",
            "Epoch [4/10] | Loss: 0.5205 | Val Acc: 0.6993\n",
            "Epoch [5/10] | Loss: 0.4552 | Val Acc: 0.7192\n",
            "Epoch [6/10] | Loss: 0.2327 | Val Acc: 0.8533\n",
            "Epoch [7/10] | Loss: 0.1211 | Val Acc: 0.8822\n",
            "Epoch [8/10] | Loss: 0.0925 | Val Acc: 0.8822\n",
            "Epoch [9/10] | Loss: 0.0671 | Val Acc: 0.8822\n",
            "Epoch [10/10] | Loss: 0.0562 | Val Acc: 0.8859\n",
            "Classifier model saved to classifier.pth\n",
            "Workflow complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qY4UXPdScmiL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}