{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "uxOuG_VSSjB3",
        "4RJhKEcYQGE8",
        "2r_bOeVIY6EZ",
        "qF6uKHWnU4ii",
        "bxwfi5dnYIPO",
        "akGvrO70YoRv",
        "6v2KPLYqZBdM",
        "XK4IExm1aFU9"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setting Up"
      ],
      "metadata": {
        "id": "s5drQ8WNM9oV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.utils.data import DataLoader, random_split, Dataset\n",
        "import torchvision.transforms.v2 as T\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50, DeepLabV3_ResNet50_Weights\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "from torchvision.datasets import OxfordIIITPet\n",
        "\n",
        "# For bounding box manipulations (if needed)\n",
        "from torchvision.ops import box_convert, box_iou\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "\n",
        "\n",
        "# Albumentations for data augmentations\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Import all the helping files\n",
        "#from DatasetLoad import get_dataloader\n",
        "#from evaluate_metrics import evaluate_segmentation_metrics"
      ],
      "metadata": {
        "id": "C3pb3fmZMbP0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23dfbda6-02ae-49c4-e361-f42457911ba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 792M/792M [00:30<00:00, 25.7MB/s]\n",
            "100%|██████████| 19.2M/19.2M [00:01<00:00, 12.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hZCYUfwUqblX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "Ig2FcMQ7LYQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Load the Oxford-IIIT Pet Dataset (Classification Labels)\n",
        "\n",
        "We only need image–class pairs, so we use:"
      ],
      "metadata": {
        "id": "ft1KI4ghLqlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import OxfordIIITPet\n",
        "\n",
        "# Custom wrapper to apply transform on-the-fly\n",
        "class TransformDataset(Dataset):\n",
        "    def __init__(self, subset, transform):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, target = self.subset[idx]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, target\n",
        "\n",
        "\n",
        "def load_classifier_dataset():\n",
        "\n",
        "    # Define transforms\n",
        "  train_transform = T.Compose([\n",
        "      T.Resize((256, 256)),\n",
        "      T.RandomResizedCrop((224, 224), scale=(0.8, 1.0)),\n",
        "      T.RandomHorizontalFlip(p=0.5),\n",
        "      T.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
        "      T.ToTensor(),\n",
        "      T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "  ])\n",
        "\n",
        "  val_transform = T.Compose([\n",
        "      T.Resize((224, 224)),\n",
        "      T.ToTensor(),\n",
        "      T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "  ])\n",
        "\n",
        "  # Load dataset without transform first\n",
        "  base_dataset = OxfordIIITPet(\n",
        "    root=\"./oxford_iiit_data\",\n",
        "    download=True,\n",
        "    target_types=\"category\",  # classification labels only\n",
        "    split=\"trainval\",\n",
        "    transform=None)\n",
        "\n",
        "\n",
        "  # Train/val split\n",
        "  train_size = int(0.85 * len(base_dataset))\n",
        "  val_size = len(base_dataset) - train_size\n",
        "  train_subset, val_subset = random_split(base_dataset, [train_size, val_size])\n",
        "\n",
        "  # Wrap subsets with respective transforms\n",
        "  train_ds = TransformDataset(train_subset, train_transform)\n",
        "  val_ds = TransformDataset(val_subset, val_transform)\n",
        "\n",
        "  # Data loaders\n",
        "  train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=2)\n",
        "  val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "  # num_classes = 37  # There are 37 pet breeds\n",
        "  return train_loader, val_loader"
      ],
      "metadata": {
        "id": "H-uxtrBBLlQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model Initialization (Classifier)\n",
        "\n",
        "Pre-trained ResNet50 for classification."
      ],
      "metadata": {
        "id": "uxOuG_VSSjB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_resnet50_classifier_model(num_classes=37, pretrained_weights=True):\n",
        "    \"\"\"\n",
        "    Oxford-IIIT has 37 categories (pet breeds) for classification.\n",
        "    \"\"\"\n",
        "    if pretrained_weights:\n",
        "      model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
        "    else:\n",
        "      model = resnet50(weights=None)\n",
        "\n",
        "    # Replace the final FC layer\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, num_classes)\n",
        "    return model"
      ],
      "metadata": {
        "id": "KoHECEcmSldX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Visualizations\n",
        "\n",
        "\n",
        "*   plot_training_curves() - to plot acc,loss,lr after training\n",
        "*   visualize_test_samples() - to visualize samples from test dataset by converting trimaps -> binary: Original image | Binary GT | Predict mask\n",
        "*   visualize_segmentation_val_ds() - to visualize samples from validation dataset: Image | Pseudo Mask | Predict Mask\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4RJhKEcYQGE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_curves(\n",
        "    num_epochs,\n",
        "    train_acc_history,\n",
        "    val_acc_history,\n",
        "    train_loss_history,\n",
        "    val_loss_history,\n",
        "    lr_per_batch,\n",
        "    prefix=\"model\",\n",
        "    show=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Creates and saves 3 separate figures:\n",
        "      1) Accuracy vs. Epoch\n",
        "      2) Loss vs. Epoch\n",
        "      3) LR vs. Batch\n",
        "\n",
        "    Inputs:\n",
        "      - num_epochs: int (total training epochs)\n",
        "      - train_acc_history: list of length num_epochs\n",
        "      - val_acc_history:   list of length num_epochs\n",
        "      - train_loss_history: list of length num_epochs\n",
        "      - val_loss_history:   list of length num_epochs\n",
        "      - lr_per_batch: list of LR values for each training batch\n",
        "      - prefix: string prefix for saving the plots\n",
        "      - show: if True, calls plt.show() for each figure\n",
        "    \"\"\"\n",
        "\n",
        "    # 1) Accuracy vs. Epoch\n",
        "    plt.figure()\n",
        "    epochs_range = range(num_epochs)\n",
        "    plt.plot(epochs_range, train_acc_history, label=\"Train Accuracy\")\n",
        "    plt.plot(epochs_range, val_acc_history,   label=\"Val Accuracy\")\n",
        "    plt.legend()\n",
        "    if prefix == 'classifier':\n",
        "      plt.title(\"Classifier Accuracy vs. No. of epochs\")\n",
        "    else:\n",
        "      plt.title(\"Segmentation Pixel Accuracy vs. No. of epochs\")\n",
        "\n",
        "\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.savefig(f\"{prefix}_accuracy_vs_epochs.png\")\n",
        "    if show:\n",
        "        plt.show()\n",
        "\n",
        "    # 2) Loss vs. Epoch\n",
        "    plt.figure()\n",
        "    plt.plot(epochs_range, train_loss_history, label=\"Train Loss\")\n",
        "    plt.plot(epochs_range, val_loss_history,   label=\"Val Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    if prefix == 'classifier':\n",
        "      plt.title(\"Classifier Loss vs. No. of epochs\")\n",
        "    else:\n",
        "      plt.title(\"Segmentation Loss vs. No. of epochs\")\n",
        "\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.savefig(f\"{prefix}_loss_vs_epochs.png\")\n",
        "    if show:\n",
        "        plt.show()\n",
        "\n",
        "    # 3) Learning Rate vs. Batch\n",
        "    plt.figure()\n",
        "    plt.plot(lr_per_batch)\n",
        "\n",
        "    if prefix == 'classifier':\n",
        "      plt.title(\"Classifier Learning Rate vs. Batch no.\")\n",
        "    else:\n",
        "      plt.title(\"Segmentation Learning Rate vs. Batch no.\")\n",
        "\n",
        "    plt.xlabel(\"Batch iteration\")\n",
        "    plt.ylabel(\"Learning Rate\")\n",
        "    plt.savefig(f\"{prefix}_lr_vs_batch.png\")\n",
        "    if show:\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "pRiw6cvDQKS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_test_samples(\n",
        "    model,\n",
        "    device,\n",
        "    dataset,\n",
        "    num_samples=5,\n",
        "    mean=(0.485, 0.456, 0.406),\n",
        "    std=(0.229, 0.224, 0.225)\n",
        "):\n",
        "    \"\"\"\n",
        "    Visualize a few samples from the test dataset:\n",
        "      - Original image (de-normalized)\n",
        "      - Ground truth mask (binary, ignoring boundary=2)\n",
        "      - Predicted mask\n",
        "\n",
        "    Args:\n",
        "        model: trained segmentation model\n",
        "        device: \"cpu\" or \"cuda\"\n",
        "        dataset: a Dataset that returns (image_tensor, trimap),\n",
        "                 e.g. your OxfordPetsSegmentation with transforms\n",
        "        num_samples: how many samples to visualize\n",
        "        mean, std: for de-normalizing images if you used ImageNet stats\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    indices = np.random.choice(len(dataset), size=num_samples, replace=False)\n",
        "\n",
        "    for idx in indices:\n",
        "        # 1) Get the sample (image, trimap)\n",
        "        image_tensor, trimap_tensor = dataset[idx]\n",
        "        # image_tensor shape: [C,H,W]\n",
        "        # trimap_tensor shape: [H,W], in {0,1,2} after any shift logic\n",
        "\n",
        "        # Move image to device, add batch dimension\n",
        "        input_batch = image_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "        # Model forward\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_batch)[\"out\"]  # [1,2,H,W] for binary\n",
        "        preds = torch.argmax(outputs, dim=1)     # [1,H,W]\n",
        "        pred_mask = preds.squeeze(0).cpu().numpy()  # [H,W] in {0,1}\n",
        "\n",
        "        # 2) Convert ground-truth trimap -> binary mask (ignore boundary=2 => set to 0 or remove)\n",
        "        trimap_np = trimap_tensor.numpy()  # [H,W]\n",
        "        # For visualization, let's just treat boundary as background or ignore.\n",
        "        # Suppose we merge boundary=2 with background=0 for the sake of display:\n",
        "        gt_mask = np.where(trimap_np == 1, 1, 0).astype(np.uint8)\n",
        "\n",
        "        # 3) De-normalize the image for display\n",
        "        #    If your dataset was normalized with mean/std, invert that:\n",
        "        denorm_image = denormalize_image(image_tensor, mean, std)\n",
        "        # shape [C,H,W], in [0,1] after clamp\n",
        "        denorm_image_np = denorm_image.permute(1,2,0).cpu().numpy()  # [H,W,C]\n",
        "\n",
        "        # 4) Plot them side by side in subplots\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "\n",
        "        # Original (de-normalized)\n",
        "        axes[0].imshow(denorm_image_np)\n",
        "        axes[0].set_title(f\"Index {idx} - Original\")\n",
        "        axes[0].axis(\"off\")\n",
        "\n",
        "        # GT Mask (binary, boundary=ignored)\n",
        "        axes[1].imshow(gt_mask, cmap=\"gray\", vmin=0, vmax=1)\n",
        "        axes[1].set_title(\"Ground Truth Mask (binary)\")\n",
        "        axes[1].axis(\"off\")\n",
        "\n",
        "        # Predicted\n",
        "        axes[2].imshow(pred_mask, cmap=\"gray\", vmin=0, vmax=1)\n",
        "        axes[2].set_title(\"Predicted Mask\")\n",
        "        axes[2].axis(\"off\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\"visualize_test_sample.png\")\n",
        "        plt.show()\n",
        "\n",
        "def denormalize_image(tensor, mean, std):\n",
        "    \"\"\"\n",
        "    Invert the normalization used for ImageNet (or custom).\n",
        "    tensor: [C,H,W], assumed normalized with mean/std\n",
        "    mean, std: tuples of length 3\n",
        "    returns a [C,H,W] in [0,1] range (clamped)\n",
        "    \"\"\"\n",
        "    tensor = tensor.clone()\n",
        "    for c in range(tensor.shape[0]):\n",
        "        tensor[c] = tensor[c]* std[c] + mean[c]\n",
        "\n",
        "    # Clamp to [0,1] to avoid any out-of-bounds\n",
        "    return torch.clamp(tensor, 0, 1)\n"
      ],
      "metadata": {
        "id": "uC_JCjUZ98DI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_segmentation_val_ds(\n",
        "    seg_model,\n",
        "    seg_val_ds,  # A Dataset that returns (image_tensor, mask_tensor)\n",
        "    device,\n",
        "    mean=(0.485, 0.456, 0.406),\n",
        "    std=(0.229, 0.224, 0.225),\n",
        "    indices_to_show=[0,1,2]\n",
        "):\n",
        "    \"\"\"\n",
        "    - seg_model: trained segmentation model (e.g. DeepLab) in eval mode\n",
        "    - seg_val_ds: dataset returning (image_tensor, mask_tensor)\n",
        "    - device: \"cuda\" or \"cpu\"\n",
        "    - mean, std: used for denormalizing the image\n",
        "    - indices_to_show: which dataset indices to visualize\n",
        "    \"\"\"\n",
        "    seg_model.eval()\n",
        "\n",
        "    for idx in indices_to_show:\n",
        "        # 1) Grab one sample (image & ground truth) from the dataset\n",
        "        image_tensor, gt_mask_tensor = seg_val_ds[idx]\n",
        "        # image_tensor: [C,H,W], already normalized\n",
        "        # gt_mask_tensor: [H,W] in {0,1} (binary) or [0..N-1] (multi-class)\n",
        "\n",
        "        # 2) Move image to device and do inference\n",
        "        input_batch = image_tensor.unsqueeze(0).to(device)  # shape [1,C,H,W]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = seg_model(input_batch)['out']  # shape [1, num_classes, H, W]\n",
        "\n",
        "        preds = torch.argmax(output, dim=1)  # shape [1,H,W]\n",
        "        pred_mask = preds.squeeze(0).cpu().numpy().astype(np.uint8)  # shape [H,W]\n",
        "\n",
        "        # 3) De-normalize the image for display\n",
        "        #    We'll convert [C,H,W] -> [H,W,C] after de-normalizing.\n",
        "        denorm_image = denormalize_image(image_tensor, mean, std)  # still [C,H,W]\n",
        "        denorm_image_np = denorm_image.permute(1,2,0).cpu().numpy()  # [H,W,C] in [0..1]\n",
        "\n",
        "        # Ground truth mask\n",
        "        gt_mask_np = gt_mask_tensor.cpu().numpy()\n",
        "\n",
        "        # 4) Plot them side by side in subplots\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "\n",
        "        # Original image\n",
        "        axes[0].imshow(denorm_image_np)\n",
        "        axes[0].set_title(f\"Original Image\")\n",
        "        axes[0].axis(\"off\")\n",
        "\n",
        "        # Ground truth mask\n",
        "        axes[1].imshow(gt_mask_np, cmap=\"gray\")\n",
        "        axes[1].set_title(\"Pseudo GT Mask (from CAM)\")\n",
        "        axes[1].axis(\"off\")\n",
        "\n",
        "        # Predicted mask\n",
        "        axes[2].imshow(pred_mask, cmap=\"gray\")\n",
        "        axes[2].set_title(\"Seg Model Predicted Mask\")\n",
        "        axes[2].axis(\"off\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "SnvfzOS-GYQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Log Metrics"
      ],
      "metadata": {
        "id": "2r_bOeVIY6EZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def log_metrics(filename, epoch, train_loss, train_acc, val_loss, val_acc, epoch_time):\n",
        "    \"\"\"\n",
        "    Appends one line of metrics to a log file.\n",
        "\n",
        "    filename: string path to the .txt file\n",
        "    epoch: current epoch number\n",
        "    train_loss, train_acc: training loss and accuracy for this epoch\n",
        "    val_loss, val_acc: validation loss and accuracy for this epoch\n",
        "    epoch_time: how long this epoch took (in seconds, for example)\n",
        "    \"\"\"\n",
        "\n",
        "    log_entry = (f\"Epoch={epoch}, \"\n",
        "                 f\"TrainLoss={train_loss:.4f}, TrainAcc={train_acc:.4f}, \"\n",
        "                 f\"ValLoss={val_loss:.4f}, ValAcc={val_acc:.4f}, \"\n",
        "                 f\"Time={epoch_time:.2f}s\")\n",
        "\n",
        "    # Append to file\n",
        "    with open(filename, \"a\") as f:\n",
        "        f.write(log_entry + \"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "y8MSRvGgY8iX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Training the Classifier\n",
        "\n",
        "We’ll include:\n",
        "  *\tLearning rate scheduling (StepLR as an example)\n",
        "  *\tWeight decay\n",
        "  *\tGradient clipping"
      ],
      "metadata": {
        "id": "qF6uKHWnU4ii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classifier_with_metrics(\n",
        "    model, train_loader, val_loader, num_epochs=10,  base_lr=1e-3, max_lr=1e-2,\n",
        "    weight_decay=1e-4, clip_grad_norm=5.0, save_path=\"classifier.pth\", log_file = \"classifier_metrics_log.txt\"):\n",
        "\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=base_lr, weight_decay=weight_decay)\n",
        "\n",
        "    # total_steps = total number of batches for the entire training\n",
        "    total_steps = len(train_loader) * num_epochs\n",
        "\n",
        "        # Initialize OneCycleLR\n",
        "    scheduler = OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=max_lr,\n",
        "        total_steps=total_steps,\n",
        "        # optional hyperparams:\n",
        "        # pct_start=0.3,  # fraction of cycle spent increasing LR\n",
        "        # anneal_strategy='cos',\n",
        "        # div_factor=10,   # initial LR = max_lr/div_factor\n",
        "        # final_div_factor=100\n",
        "    )\n",
        "\n",
        "    train_loss_history = []\n",
        "    val_loss_history   = []\n",
        "    train_acc_history  = []\n",
        "    val_acc_history    = []\n",
        "    lr_per_batch       = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in train_loader: # bboxes not loaded into train loader so not used in training here\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            if clip_grad_norm is not None:\n",
        "              torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_grad_norm)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            # Step the OneCycle scheduler **after** the optimizer step\n",
        "            scheduler.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            lr_per_batch.append(optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_train_acc  = correct / total\n",
        "        train_loss_history.append(epoch_train_loss)\n",
        "        train_acc_history.append(epoch_train_acc)\n",
        "\n",
        "        val_loss, val_acc = evaluate_classifier_with_accuracy(model, val_loader, criterion, device)\n",
        "        val_loss_history.append(val_loss)\n",
        "        val_acc_history.append(val_acc)\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "\n",
        "        # log metrics\n",
        "        log_metrics(\n",
        "            filename=log_file,\n",
        "            epoch=epoch+1,\n",
        "            train_loss=epoch_train_loss,\n",
        "            train_acc=epoch_train_acc,\n",
        "            val_loss=val_loss,\n",
        "            val_acc=val_acc,\n",
        "            epoch_time=epoch_time)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
        "              f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"Classifier model saved to {save_path}\")\n",
        "\n",
        "\n",
        "    # Call the common plotting function\n",
        "    plot_training_curves(\n",
        "        num_epochs,\n",
        "        train_acc_history,\n",
        "        val_acc_history,\n",
        "        train_loss_history,\n",
        "        val_loss_history,\n",
        "        lr_per_batch,\n",
        "        prefix=\"classifier\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def evaluate_classifier_with_accuracy(model, val_loader, criterion, device):\n",
        "    \"\"\"Returns val_loss, val_accuracy in training loop.\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total   += labels.size(0)\n",
        "\n",
        "    val_loss = running_loss / len(val_loader.dataset)\n",
        "    val_acc  = correct / total\n",
        "    return val_loss, val_acc"
      ],
      "metadata": {
        "id": "hs_t-lAoPFt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Generating Raw CAMs (Grad-CAM)\n",
        "\n",
        "A straightforward Grad-CAM approach:"
      ],
      "metadata": {
        "id": "bxwfi5dnYIPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GradCAM:\n",
        "    \"\"\"\n",
        "    Simple Grad-CAM for ResNet-based networks.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, target_layer_name=\"layer4\"):\n",
        "        self.model = model\n",
        "        self.model.eval()\n",
        "\n",
        "        # Hook the target layer\n",
        "        self.target_layer = None\n",
        "        for name, module in self.model.named_children():\n",
        "            if name == target_layer_name:\n",
        "                self.target_layer = module\n",
        "                break\n",
        "\n",
        "        if self.target_layer is None:\n",
        "            raise ValueError(f\"Layer {target_layer_name} not found in model\")\n",
        "\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations = output\n",
        "\n",
        "        def backward_hook(module, grad_in, grad_out):\n",
        "            self.gradients = grad_out[0]\n",
        "\n",
        "        self.target_layer.register_forward_hook(forward_hook)\n",
        "        self.target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "    def __call__(self, x, class_idx=None):\n",
        "        \"\"\"\n",
        "        x: input image tensor of shape [B, C, H, W]\n",
        "        class_idx: which class index to compute CAM for. If None, uses argmax\n",
        "        Returns: CAM for each image in the batch\n",
        "        \"\"\"\n",
        "        logits = self.model(x)  # forward pass\n",
        "        if class_idx is None:\n",
        "            class_idx = torch.argmax(logits, dim=1)\n",
        "\n",
        "        # Compute gradients w.r.t. target class\n",
        "        one_hot = torch.zeros_like(logits)\n",
        "        for i in range(logits.size(0)):\n",
        "            one_hot[i, class_idx[i]] = 1.0\n",
        "\n",
        "        self.model.zero_grad()\n",
        "        logits.backward(gradient=one_hot, retain_graph=True)\n",
        "\n",
        "        gradients = self.gradients  # [B, C, H', W']\n",
        "        activations = self.activations  # [B, C, H', W']\n",
        "\n",
        "        # Global-average-pool the gradients\n",
        "        weights = torch.mean(gradients, dim=(2, 3), keepdim=True)  # [B, C, 1, 1]\n",
        "\n",
        "        # Weighted sum of activations\n",
        "        cams = (weights * activations).sum(dim=1, keepdim=True)  # [B, 1, H', W']\n",
        "\n",
        "        # ReLU\n",
        "        cams = F.relu(cams)\n",
        "\n",
        "        # Normalize each CAM to [0,1]\n",
        "        cams = cams - cams.view(cams.size(0), -1).min(dim=1)[0].view(cams.size(0),1,1,1)\n",
        "        cams = cams / (cams.view(cams.size(0), -1).max(dim=1)[0].view(cams.size(0),1,1,1) + 1e-8)\n",
        "\n",
        "        return cams"
      ],
      "metadata": {
        "id": "GF0Nt5awYMJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bounding Box Usage:**\n",
        "  *\tBounding box expansion helps ensure that most of the pet is inside the bounding box.\n",
        "  *\tSoft weighting ensures that if there’s any part of the pet outside that expanded box, you don’t completely discard it (i.e., zero it out). Instead, you downweight it, but you still keep it if the model strongly activates there."
      ],
      "metadata": {
        "id": "sPQm8-2IYPve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi_Scale_CAM**:\n",
        "\n",
        "multi_scale_cam loops over each scale in scales, resizing the input images, calling gradcam(...), and then upsampling the resulting CAM back to the original shape.\n",
        "\n",
        "We stack and average them, producing a single combined CAM for each image in the batch."
      ],
      "metadata": {
        "id": "ZwuqbE4gpw4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def multi_scale_cam(gradcam, images, class_idx, scales=[0.75, 1.0, 1.25]):\n",
        "    \"\"\"\n",
        "    images: [B, C, H, W] in torch format\n",
        "    class_idx: [B] (which class to compute CAM for each image)\n",
        "    scales: list of float scaling factors\n",
        "    returns: a single combined CAM per image [B,1,H,W] in [0,1]\n",
        "    \"\"\"\n",
        "    device = images.device\n",
        "    B, C, H, W = images.shape\n",
        "\n",
        "    all_cams = []\n",
        "    for scale in scales:\n",
        "        # 1) Resize images\n",
        "        new_h = int(H * scale)\n",
        "        new_w = int(W * scale)\n",
        "        scaled_imgs = F.interpolate(images, size=(new_h, new_w),\n",
        "                                    mode='bilinear', align_corners=False)\n",
        "\n",
        "        # 2) Run GradCAM for each scaled image\n",
        "        with torch.enable_grad():\n",
        "            scaled_cams = gradcam(scaled_imgs, class_idx=class_idx)\n",
        "            # scaled_cams shape: [B, 1, new_h', new_w']\n",
        "\n",
        "        # 3) Upsample each CAM back to original (H,W)\n",
        "        up_cams = F.interpolate(scaled_cams, size=(H, W),\n",
        "                                mode='bilinear', align_corners=False)\n",
        "        all_cams.append(up_cams)\n",
        "\n",
        "    # 4) Average all scaled CAMs\n",
        "    combined = torch.mean(torch.stack(all_cams, dim=0), dim=0)  # shape [B,1,H,W]\n",
        "    combined = torch.clamp(combined, 0, 1)  # ensure [0,1]\n",
        "    return combined\n"
      ],
      "metadata": {
        "id": "pXM-vZE0pPra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_bbox(bbox, expansion_ratio=1.5, img_w=224, img_h=224):\n",
        "    \"\"\"\n",
        "    Expand the bounding box by 'expansion_ratio' while staying within [0, img_w]x[0, img_h].\n",
        "    bbox: (x_min, y_min, x_max, y_max)\n",
        "    expansion_ratio: e.g., 1.5 -> 50% bigger in width/height each side.\n",
        "    \"\"\"\n",
        "    x_min, y_min, x_max, y_max = bbox\n",
        "\n",
        "    # Current width/height\n",
        "    box_w = x_max - x_min\n",
        "    box_h = y_max - y_min\n",
        "\n",
        "    # Expansion amount\n",
        "    dw = (box_w * (expansion_ratio - 1)) / 2.0\n",
        "    dh = (box_h * (expansion_ratio - 1)) / 2.0\n",
        "\n",
        "    # Center-based expansion\n",
        "    cx = 0.5 * (x_min + x_max)\n",
        "    cy = 0.5 * (y_min + y_max)\n",
        "\n",
        "    new_xmin = max(0, int(cx - (box_w/2 + dw)))\n",
        "    new_xmax = min(img_w, int(cx + (box_w/2 + dw)))\n",
        "    new_ymin = max(0, int(cy - (box_h/2 + dh)))\n",
        "    new_ymax = min(img_h, int(cy + (box_h/2 + dh)))\n",
        "\n",
        "    return (new_xmin, new_ymin, new_xmax, new_ymax)\n",
        "\n",
        "def apply_expanded_bbox_soft_weights(cam, bbox, H, W,\n",
        "                                     expansion_ratio=1.5,\n",
        "                                     inside_weight=0.9,\n",
        "                                     outside_weight=0.3):\n",
        "    \"\"\"\n",
        "    1) Expand the bounding box by 'expansion_ratio'.\n",
        "    2) Apply soft weighting inside vs. outside the expanded box.\n",
        "       - inside_weight = 0.9\n",
        "       - outside_weight = 0.3 (or any fraction you want)\n",
        "    \"\"\"\n",
        "    # Expand the bbox\n",
        "    x_min, y_min, x_max, y_max = expand_bbox(bbox, expansion_ratio, img_w=W, img_h=H)\n",
        "\n",
        "    # Create a weight map\n",
        "    weights = np.full((H, W), outside_weight, dtype=np.float32)\n",
        "    weights[y_min:y_max, x_min:x_max] = inside_weight\n",
        "\n",
        "    return cam * weights\n",
        "\n",
        "def generate_cams(model, data_loader, gradcam, device,\n",
        "                  apply_bbox=True, output_dir=\"cams_out\", scales=[0.75, 1.0, 1.25]):\n",
        "    \"\"\"\n",
        "    Generate and save CAMs.\n",
        "    We'll upsample the CAM to the original image size,\n",
        "    then optionally mask it with the bounding box.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    model.eval()\n",
        "    gradcam.model.eval()\n",
        "\n",
        "    #idx = 0  # for unique file naming\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, targets, paths) in enumerate(data_loader):\n",
        "            # images shape: [B, C, H, W]\n",
        "            images = images.to(device)\n",
        "\n",
        "            # You need a class index to call gradcam(..., class_idx=label)\n",
        "            # If you have exactly one label per image, do something like:\n",
        "            # (assuming the target is detection style but you only want label[0])\n",
        "            class_idxs = []\n",
        "            for t in targets:\n",
        "                # If there's exactly 1 label per image\n",
        "                class_idxs.append(t[\"labels\"][0])\n",
        "\n",
        "            class_idxs = torch.tensor(class_idxs, dtype=torch.long, device=device)\n",
        "\n",
        "            # GradCAM call needs a forward and backward pass,\n",
        "            # so let's do it with requires_grad\n",
        "            with torch.enable_grad():\n",
        "                # cams_batch = gradcam(images, class_idx=class_idxs)\n",
        "                cams_batch = multi_scale_cam(gradcam, images, class_idx=class_idxs, scales=scales)\n",
        "\n",
        "\n",
        "            # cams_batch shape: [B, 1, H', W']\n",
        "            # Upsample CAM to match input size (e.g. 224x224 if that's your input transform)\n",
        "            # using bilinear interpolation\n",
        "            upsampled_cams = F.interpolate(cams_batch, size=(images.shape[2], images.shape[3]),\n",
        "                                           mode='bilinear', align_corners=False)\n",
        "\n",
        "            # Now upsampled_cams is [B, 1, H, W]\n",
        "            upsampled_cams = upsampled_cams.squeeze(1).cpu().numpy()  # shape [B, H, W]\n",
        "\n",
        "            # If original images are bigger than 224, you may have to re-scale bounding boxes and\n",
        "            # possibly re-scale the CAM again. This depends on your transformations.\n",
        "\n",
        "            # Loop over each image in the batch\n",
        "            for b in range(len(images)):\n",
        "                cam_2d = upsampled_cams[b]\n",
        "\n",
        "\n",
        "                if apply_bbox:\n",
        "                    # bboxes[b] is the bounding box for this image in original scale\n",
        "                    # Make sure the scale is consistent with your input size!\n",
        "\n",
        "                    # We might have multiple boxes. Suppose we only want box 0:\n",
        "                    bbox = targets[b][\"boxes\"][0]  # shape [4]\n",
        "                    # If needed, convert to numpy\n",
        "                    bbox = bbox.cpu().numpy()\n",
        "\n",
        "                    cam_2d = apply_expanded_bbox_soft_weights(\n",
        "                                cam_2d,\n",
        "                                bbox,\n",
        "                                H=cam_2d.shape[0],\n",
        "                                W=cam_2d.shape[1],\n",
        "                                expansion_ratio=1.5,\n",
        "                                inside_weight=1.0,\n",
        "                                outside_weight=0.6\n",
        "                            )\n",
        "\n",
        "                # Build a filename from the image path\n",
        "                base_name = os.path.splitext(os.path.basename(paths[b]))[0]\n",
        "                cam_file = f\"{base_name}_cam.npy\"\n",
        "                cam_path = os.path.join(output_dir, cam_file)\n",
        "\n",
        "                # Save each CAM\n",
        "                # cam_path = os.path.join(output_dir, f\"cam_{idx}.npy\")\n",
        "                np.save(cam_path, cam_2d)\n",
        "                #idx += 1\n",
        "\n",
        "    print(\"CAM generation complete!\")"
      ],
      "metadata": {
        "id": "IVFw4lnRYeUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Apply ReCAM (Refinement / Expansion)\n",
        "\n",
        "Below is an illustrative approach inspired by the ReCAM paper. This usually involves:\n",
        "  *\tRe-scoring the CAM to ensure more complete coverage of the object.\n",
        "  *\tPossibly iterative expansions (e.g. random erasing, multi-scale expansions).\n",
        "  *\tWe show a simplified version that scales up smaller areas, and we add expansions."
      ],
      "metadata": {
        "id": "akGvrO70YoRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recam_refinement(cam, expansion_factor=1.2, threshold=0.3):\n",
        "    \"\"\"\n",
        "    Simplified approach:\n",
        "    1. If the average of the top region is below a certain threshold,\n",
        "       push it up (expand coverage).\n",
        "    2. You could also do iterative random erasing or multi-scale expansions.\n",
        "    \"\"\"\n",
        "    # cam: 2D np array [H, W] in [0,1]\n",
        "    # Step 1: thresholding\n",
        "    mask = (cam >= threshold).astype(np.uint8)\n",
        "    coverage = mask.sum() / (cam.shape[0]*cam.shape[1])\n",
        "\n",
        "    # If coverage < some ratio, inflate the activation\n",
        "    if coverage < 0.1:\n",
        "        cam = cam * expansion_factor\n",
        "        cam = np.clip(cam, 0, 1)\n",
        "\n",
        "    # Re-threshold\n",
        "    return cam\n",
        "\n",
        "def refine_cams_with_recam(cam_dir, refined_dir=\"cams_refined\",\n",
        "                           threshold=0.3, expansion_factor=1.2):\n",
        "    os.makedirs(refined_dir, exist_ok=True)\n",
        "\n",
        "    cam_files = [f for f in os.listdir(cam_dir) if f.endswith('.npy')]\n",
        "    for cfile in cam_files:\n",
        "        cam_path = os.path.join(cam_dir, cfile)\n",
        "        cam = np.load(cam_path)\n",
        "\n",
        "        refined_cam = recam_refinement(cam, expansion_factor, threshold)\n",
        "\n",
        "        # Save\n",
        "        refined_path = os.path.join(refined_dir, cfile)  # same base name\n",
        "        np.save(refined_path, refined_cam)\n",
        "\n",
        "    print(\"ReCAM refinement complete!\")"
      ],
      "metadata": {
        "id": "izWbRKYZY8bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def recam_refinement_extended(cam,\n",
        "#                               iteration_steps=2,\n",
        "#                               erase_ratio=0.15,   # smaller\n",
        "#                               morph_kernel_size=5,\n",
        "#                               coverage_threshold=0.2,\n",
        "#                               expansion_factor=1.1,  # smaller expansions\n",
        "#                               morphological_threshold=0.2  # also lower\n",
        "#                              ):\n",
        "#     h, w = cam.shape\n",
        "#     refined = cam.copy()\n",
        "\n",
        "#     for it in range(iteration_steps):\n",
        "#         # top X% region\n",
        "#         flattened = refined.flatten()\n",
        "#         sort_idx = np.argsort(flattened)[::-1]\n",
        "#         num_erase = int(len(flattened) * erase_ratio)\n",
        "#         if num_erase < 1:\n",
        "#             break\n",
        "\n",
        "#         top_indices = sort_idx[:num_erase]\n",
        "\n",
        "#         # Erase by scaling them to 0.2 instead of 0.1 or 0.0\n",
        "#         refined_flat = refined.flatten()\n",
        "#         refined_flat[top_indices] = refined_flat[top_indices] * 0.2\n",
        "#         refined = refined_flat.reshape(h, w)\n",
        "\n",
        "#         # Morphological expansion with a lower threshold\n",
        "#         refined = morphological_expand(refined,\n",
        "#                                        kernel_size=morph_kernel_size,\n",
        "#                                        bin_thresh=morphological_threshold)\n",
        "\n",
        "#         # Coverage\n",
        "#         coverage = np.mean(refined > morphological_threshold)\n",
        "#         if coverage < coverage_threshold:\n",
        "#             refined *= expansion_factor\n",
        "#             refined = np.clip(refined, 0, 1.0)\n",
        "\n",
        "#     refined = np.clip(refined, 0, 1.0)\n",
        "#     return refined\n",
        "\n",
        "\n",
        "# def morphological_expand(cam, kernel_size=5, bin_thresh=0.2):\n",
        "#     bin_map = (cam >= bin_thresh).astype(np.uint8)\n",
        "#     kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))\n",
        "#     dilated = cv2.dilate(bin_map, kernel, iterations=1)\n",
        "#     expanded = cam.copy()\n",
        "#     # Raise them to 0.3 or so\n",
        "#     expanded[dilated == 1] = np.maximum(expanded[dilated == 1], 0.3)\n",
        "#     return expanded\n",
        "\n",
        "\n",
        "# def refine_cams_with_recam_extended(input_cam_dir, output_dir=\"cams_refined\", iteration_steps=2, erase_ratio=0.15,\n",
        "#                                     threshold=0.2, expansion_factor=1.1):\n",
        "#     import os\n",
        "#     os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "#     for f in os.listdir(input_cam_dir):\n",
        "#         if not f.endswith(\".npy\"):\n",
        "#             continue\n",
        "#         path_in = os.path.join(input_cam_dir, f)\n",
        "#         cam = np.load(path_in)  # shape [H,W], float in [0,1]\n",
        "\n",
        "#         # Some bounding box weighting if desired\n",
        "#         # e.g., cam = apply_bbox_soft_weights(cam, bboxes, inside_weight=1.0, outside_weight=0.3)\n",
        "\n",
        "#         # Extended ReCAM\n",
        "#         refined = recam_refinement_extended(\n",
        "#             cam,\n",
        "#             iteration_steps=iteration_steps,\n",
        "#             erase_ratio=erase_ratio,\n",
        "#             morph_kernel_size=5,\n",
        "#             coverage_threshold=threshold,\n",
        "#             expansion_factor=1.2\n",
        "#         )\n",
        "\n",
        "#         out_path = os.path.join(output_dir, f)\n",
        "#         np.save(out_path, refined)\n",
        "\n",
        "#     print(\"Extended ReCAM refinement complete!\")"
      ],
      "metadata": {
        "id": "hEjGvt3IqzI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Pseudo-Label Filtering\n",
        "\n",
        "A simple approach:\n",
        "* We binarize or do top-k%.\n",
        "* For example, if we do a threshold t=0.5, anything above 0.5 = foreground, else background."
      ],
      "metadata": {
        "id": "6v2KPLYqZBdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_pseudo_masks(refined_cam_dir, output_mask_dir=\"pseudo_masks\", threshold=0.3):\n",
        "    os.makedirs(output_mask_dir, exist_ok=True)\n",
        "\n",
        "    cam_files = [f for f in os.listdir(refined_cam_dir) if f.endswith('.npy')]\n",
        "\n",
        "    for cfile in cam_files:\n",
        "        cam_path = os.path.join(refined_cam_dir, cfile)\n",
        "        cam = np.load(cam_path)\n",
        "\n",
        "        # Binarize\n",
        "        pseudo_mask = (cam >= threshold).astype(np.uint8)\n",
        "\n",
        "        # Save as PNG, for instance (Convert \"myImage_cam.npy\" -> \"myImage_cam.png\")\n",
        "        mask_path = os.path.join(output_mask_dir, cfile.replace('.npy', '.png'))\n",
        "        cv2.imwrite(mask_path, pseudo_mask*255)\n",
        "\n",
        "    print(\"Pseudo-label generation complete!\")"
      ],
      "metadata": {
        "id": "id8ylnbEZIXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Train Segmentation Model (DeepLab V3+)\n",
        "Now we use the generated pseudo masks as “ground truth” for training. We’ll assume you have a new dataset that loads:\n",
        "  *\t(Image, PseudoMask)\n",
        "  *\tPossibly ignoring bounding boxes at this stage."
      ],
      "metadata": {
        "id": "JVKQ33-LZRcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transforms:\n",
        "Explanation\n",
        "*\tResize(256, 256): Brings the image + mask to a consistent scale before further augmentations.\n",
        "* RandomCrop(224, 224): Crops both the image + mask to 224×224 at the same random location.\n",
        "* HorizontalFlip(p=0.5): Flips the image + mask horizontally 50% of the time.\n",
        "* ColorJitter: Adjusts brightness, contrast, saturation, and hue of the image only. Albumentations automatically knows to skip these color transformations on the mask.\n",
        "* Normalize: Normalizes the image using mean & std from ImageNet. The mask is unaffected.\n",
        "* ToTensorV2: Converts both the image (float tensor in [C, H, W] with normalization) and the mask (uint8 or float tensor in [H, W]) to PyTorch tensors.\n",
        "\n",
        "Different Transforms for Images vs. Masks?\n",
        "* Geometric transforms (resize, flip, crop) must be applied equally to both the image and the segmentation mask. Otherwise, they would no longer be aligned.\n",
        "* Color transforms should only be applied to the image—the mask is a label map, so color transformations don’t make sense. Albumentations handles that automatically: if you mark the mask as \"mask\", it only applies geometric transforms to the mask.\n",
        "* Typically, you do not need separate pipelines for images and masks. You define a single Albumentations Compose with additional_targets={\"mask\": \"mask\"}, which ensures the correct behavior for both.\n",
        "\n",
        "Hence, you do not build two separate transformations. Instead, you define one Albumentations pipeline that handles each item differently depending on whether it’s the \"image\" or \"mask\" key.\n"
      ],
      "metadata": {
        "id": "fofKLWWIqGwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------------------#\n",
        "# from DatasetLoad.py to load the Oxford-IIIT Pet dataset with bounding boxes and apply basic transformations using albumentations\n",
        "#--------------------------#\n",
        "\n",
        "root_dir = \"./oxford_iiit_data\"  # or any folder you choose\n",
        "\n",
        "dummy_dataload = torchvision.datasets.OxfordIIITPet(\n",
        "    root=root_dir,\n",
        "    split=\"trainval\",\n",
        "    target_types=[\"category\"],  # or [\"segmentation\"] / [\"category\", \"segmentation\"]\n",
        "    download=True,\n",
        ")\n",
        "\n",
        "def parse_xml_for_bbox(xml_file):\n",
        "    \"\"\"Parse the Oxford-IIIT Pet XML file for bounding box [xmin, ymin, xmax, ymax].\"\"\"\n",
        "    if not os.path.exists(xml_file):\n",
        "        # Return None or raise an exception.\n",
        "        # For skipping, you could do:\n",
        "        return None, None  # Indicate missing\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    bboxes = []\n",
        "    cls = []\n",
        "    for obj in root.findall(\"object\"):\n",
        "        name = obj.find(\"name\").text\n",
        "        bbox = obj.find(\"bndbox\")\n",
        "        xmin = float(bbox.find(\"xmin\").text)\n",
        "        ymin = float(bbox.find(\"ymin\").text)\n",
        "        xmax = float(bbox.find(\"xmax\").text)\n",
        "        ymax = float(bbox.find(\"ymax\").text)\n",
        "\n",
        "        bboxes.append([xmin, ymin, xmax, ymax])\n",
        "        cls.append(name)\n",
        "\n",
        "    return bboxes, cls\n",
        "\n",
        "def read_split_file(file_path):\n",
        "    \"\"\"\n",
        "    Reads lines from trainval.txt or test.txt.\n",
        "    Returns a list of image IDs (strings).\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r') as f:\n",
        "        lines = f.read().strip().split('\\n')\n",
        "    # Each line might look like \"Abyssinian_1 1 1\"\n",
        "    # We'll take the first token as the image base name and 2nd token as breed-class\n",
        "    name= [line.split()[0] for line in lines if line.strip()]\n",
        "    breed= [line.split()[1] for line in lines if line.strip()]\n",
        "    return name, breed\n",
        "\n",
        "def build_annotations_list(root_dir, txt_filename):\n",
        "    \"\"\"\n",
        "    Build a list of annotation dicts from trainval.txt or test.txt.\n",
        "    Each dict might look like:\n",
        "        {\n",
        "          'image_id': 'Abyssinian_1.jpg',\n",
        "          'boxes': [[xmin, ymin, xmax, ymax]],    # can have multiple\n",
        "          'labels': ['Abyssinian']                # or numeric label if you prefer\n",
        "        }\n",
        "    \"\"\"\n",
        "    images_dir = os.path.join(root_dir, \"images\")\n",
        "    xmls_dir   = os.path.join(root_dir, \"annotations\", \"xmls\")\n",
        "    txt_path   = os.path.join(root_dir, \"annotations\", txt_filename)\n",
        "\n",
        "    image_bases, labels = read_split_file(txt_path)\n",
        "\n",
        "    annotation_dicts = []\n",
        "    for base in image_bases:\n",
        "        xml_file  = os.path.join(xmls_dir, f\"{base}.xml\")\n",
        "        image_file = os.path.join(images_dir, f\"{base}.jpg\")\n",
        "        label = labels[image_bases.index(base)]\n",
        "\n",
        "        bboxes, cls = parse_xml_for_bbox(xml_file)\n",
        "        if bboxes is None or cls is None:\n",
        "            # Means parse_xml_for_bbox returned None -> skip\n",
        "            print(f\"Skipping missing file: {xml_file}\")\n",
        "            continue\n",
        "\n",
        "        # Also check if image_file actually exists\n",
        "        if not os.path.exists(image_file):\n",
        "            print(f\"Skipping missing image: {image_file}\")\n",
        "            continue\n",
        "\n",
        "        annotation_dicts.append({\n",
        "            'image_file': image_file,\n",
        "            'boxes': bboxes,       # list of [xmin, ymin, xmax, ymax]\n",
        "            'labels': label       # breeds (classes indexed - [1,1,1...,2,2,2..3..])\n",
        "        })\n",
        "    return annotation_dicts\n",
        "\n",
        "def load_oxford_pet_annotations(root_dir):\n",
        "    \"\"\"\n",
        "    Loads 'trainval.txt' and 'test.txt' from the dataset folder, returning\n",
        "    two lists of annotation dicts.\n",
        "    \"\"\"\n",
        "    trainval_anns = build_annotations_list(root_dir, \"trainval.txt\")\n",
        "    return trainval_anns\n",
        "\n",
        "class OxfordPetBboxDatasetAlbumentations(torch.utils.data.Dataset):\n",
        "    def __init__(self, annotations, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            annotations: list of dicts, each like:\n",
        "                {\n",
        "                  'image_file': '/path/to/img.jpg',\n",
        "                  'boxes': [[xmin, ymin, xmax, ymax], ...],\n",
        "                  'labels': ['1', ['2']...]\n",
        "                }\n",
        "            transform: An Albumentations transform pipeline.\n",
        "        \"\"\"\n",
        "        self.annotations = annotations\n",
        "        self.transform = transform\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ann = self.annotations[idx]\n",
        "        img_path = ann['image_file']\n",
        "        boxes = ann['boxes']  # List of bounding boxes in pascal_voc format\n",
        "        labels = ann['labels']  # List of label strings\n",
        "\n",
        "        # Load image as a NumPy array.\n",
        "        image = np.array(Image.open(img_path).convert('RGB'))\n",
        "\n",
        "        # Duplicate labels for each bounding box to match the length of bboxes\n",
        "        labels = [labels] * len(boxes)  # This now creates a list of labels\n",
        "\n",
        "\n",
        "        if self.transform:\n",
        "            # Albumentations expects keys: image, bboxes, labels.\n",
        "            # It will output a dict with keys: image, bboxes, labels.\n",
        "            transformed = self.transform(image=image, bboxes=boxes, labels=labels)\n",
        "            image = transformed['image']\n",
        "            boxes = transformed['bboxes']\n",
        "            labels = transformed['labels']\n",
        "\n",
        "\n",
        "\n",
        "        # Convert bounding boxes to a tensor.\n",
        "        boxes_tensor = torch.tensor(boxes, dtype=torch.float32)\n",
        "        # pick from list, change type to int and -1 so that [1..37] --> [0..36] class labels\n",
        "        labels_tensor = torch.tensor([int(int(l)-1) for l in labels], dtype=torch.int64)\n",
        "\n",
        "\n",
        "        return image, {\"boxes\": boxes_tensor, \"labels\": labels_tensor}, img_path\n",
        "\n",
        "albumentations_transform = A.Compose(\n",
        "    [\n",
        "        A.Resize(224, 224), # Resize to 224x224\n",
        "        A.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                    std=(0.229, 0.224, 0.225)),\n",
        "        ToTensorV2(),  # Converts the image to a PyTorch tensor\n",
        "    ],\n",
        "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'])\n",
        ")\n",
        "\n",
        "\n",
        "def detection_collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function for object detection.\n",
        "\n",
        "    Args:\n",
        "        batch: list of tuples (image_tensor, target_dict)\n",
        "\n",
        "    Returns:\n",
        "        batch_images: Tensor [B, C, H, W]\n",
        "        batch_targets: list of target dicts (each with 'boxes' and 'labels')\n",
        "    \"\"\"\n",
        "    images = [item[0] for item in batch]\n",
        "    targets = [item[1] for item in batch]\n",
        "    paths = [item[2] for item in batch]\n",
        "    images = torch.stack(images, dim=0)  # This works because all images are resized\n",
        "\n",
        "    return images, targets, paths\n",
        "\n",
        "def get_dataloader():\n",
        "    trainval_anns = load_oxford_pet_annotations(\"/content/oxford_iiit_data/oxford-iiit-pet\")\n",
        "    trainval_dataset = OxfordPetBboxDatasetAlbumentations(trainval_anns, transform=albumentations_transform)\n",
        "\n",
        "    img_paths = []\n",
        "    for i in range(len(trainval_anns)):\n",
        "        img_paths.append(trainval_anns[i]['image_file'])\n",
        "\n",
        "    # DataLoader\n",
        "    train_loader = DataLoader(trainval_dataset, batch_size=8, shuffle=False,  num_workers=2, collate_fn=detection_collate_fn)\n",
        "    return train_loader, img_paths"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "rOrCLXlYOPnb",
        "outputId": "9484d1b3-991c-48bb-9acf-67879d5ce3b0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torchvision' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-76ff14641583>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mroot_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./oxford_iiit_data\"\u001b[0m  \u001b[0;31m# or any folder you choose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m dummy_dataload = torchvision.datasets.OxfordIIITPet(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"trainval\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torchvision' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "seg_transform = A.Compose([\n",
        "    # 1) Resize - to have a consistent baseline size\n",
        "    A.Resize(224, 224),\n",
        "\n",
        "    # Removed RandomCrop\n",
        "\n",
        "    # 2) Horizontal Flip (50% chance)\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "\n",
        "    # 3) Optional color augmentations that affect ONLY the image\n",
        "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
        "\n",
        "    # 4) Normalize the image (typical ImageNet stats)\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                std=(0.229, 0.224, 0.225)),\n",
        "\n",
        "    # 5) Convert both image and mask to torch tensors\n",
        "    ToTensorV2()\n",
        "],\n",
        "additional_targets={\"mask\": \"mask\"})"
      ],
      "metadata": {
        "id": "wU-T4IDxpBFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def build_img_mask_pairs(img_dir, mask_dir, img_paths):\n",
        "    \"\"\"\n",
        "    Given the original list of image_paths (which you used in the dataset),\n",
        "    build a (image_path, mask_path) list. We assume the mask is named:\n",
        "      <image_basename>_cam.png\n",
        "    in mask_dir.\n",
        "    \"\"\"\n",
        "    pairs = []\n",
        "    for ipath in img_paths:\n",
        "        base_name = os.path.splitext(os.path.basename(ipath))[0]\n",
        "        mask_name = f\"{base_name}_cam.png\"\n",
        "        mask_path = os.path.join(mask_dir, mask_name)\n",
        "        pairs.append((ipath, mask_path))\n",
        "    return pairs"
      ],
      "metadata": {
        "id": "oY-8XWFvlJL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PseudoSegDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, img_mask_pairs, transform=None):\n",
        "        self.img_mask_pairs = img_mask_pairs # list of (image_path, mask_path)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_mask_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, mask_path = self.img_mask_pairs[idx]\n",
        "\n",
        "        # Load image\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.resize(image, (224, 224), interpolation=cv2.INTER_LINEAR)    # Now image is guaranteed 224×224\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "\n",
        "        # Load pseudo mask\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)   # 0/255\n",
        "        mask = (mask > 127).astype(np.uint8) # binarize as 0/1\n",
        "\n",
        "\n",
        "        if self.transform:\n",
        "            # Albumentations expects 'image' and 'mask' in a dictionary\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image = augmented[\"image\"]\n",
        "            mask = augmented[\"mask\"]\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "\n",
        "def get_deeplab_v3(num_classes=2, pretrained_weights=True):\n",
        "\n",
        "    if pretrained_weights:\n",
        "      model = deeplabv3_resnet50(weights=DeepLabV3_ResNet50_Weights.DEFAULT)\n",
        "    else:\n",
        "      model = deeplabv3_resnet50(weights=None, weights_backbone=None, progress=False)\n",
        "\n",
        "    model.classifier[4] = nn.Conv2d(256, num_classes, kernel_size=1)  # output channels = 2 (background, foreground)\n",
        "    return model"
      ],
      "metadata": {
        "id": "Cei629m1Zdyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training loop for segmentation:**"
      ],
      "metadata": {
        "id": "G4yUqnCnZjic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_segmentation_with_metrics(\n",
        "    seg_model,\n",
        "    seg_train_loader,\n",
        "    seg_val_loader,\n",
        "    num_epochs=10,\n",
        "    base_lr=1e-3,\n",
        "    max_lr=1e-2,\n",
        "    weight_decay=1e-4,\n",
        "    save_path=\"seg_model.pth\", log_file = \"segmentation_metrics_log.txt\"):\n",
        "\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    seg_model = seg_model.to(device)\n",
        "\n",
        "    optimizer = optim.Adam(seg_model.parameters(), lr=base_lr, weight_decay=weight_decay)\n",
        "\n",
        "    total_steps = len(seg_train_loader) * num_epochs\n",
        "\n",
        "    scheduler = OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=max_lr,\n",
        "        total_steps=total_steps\n",
        "    )\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()  # or BCEWithLogitsLoss for binary\n",
        "\n",
        "    train_loss_history = []\n",
        "    val_loss_history   = []\n",
        "    train_acc_history  = []\n",
        "    val_acc_history    = []\n",
        "    lr_per_batch       = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        seg_model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_px   = 0\n",
        "        total_px     = 0\n",
        "\n",
        "        for images, masks in seg_train_loader:\n",
        "            images = images.to(device)    # [B, C, H, W]\n",
        "            masks  = masks.long().to(device)  # [B, H, W] in {0,1} for binary\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = seg_model(images)['out']  # [B, 2, H, W] if binary\n",
        "            loss = criterion(outputs, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            scheduler.step()  # step after each batch\n",
        "\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "            # Pixel-wise accuracy (for binary, predicted label is argmax among [0,1])\n",
        "            preds = torch.argmax(outputs, dim=1)  # [B, H, W]\n",
        "            correct_px += (preds == masks).sum().item()\n",
        "            total_px   += masks.numel()\n",
        "\n",
        "            current_lr = optimizer.param_groups[0][\"lr\"]\n",
        "            lr_per_batch.append(current_lr)\n",
        "\n",
        "        train_loss = running_loss / len(seg_train_loader.dataset)\n",
        "        train_acc  = correct_px / total_px  # pixel accuracy\n",
        "        train_loss_history.append(train_loss)\n",
        "        train_acc_history.append(train_acc)\n",
        "\n",
        "        # Validation\n",
        "        val_loss, val_acc = evaluate_segmentation_with_accuracy(seg_model, seg_val_loader, criterion, device)\n",
        "        val_loss_history.append(val_loss)\n",
        "        val_acc_history.append(val_acc)\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "        # log metrics\n",
        "        log_metrics(\n",
        "            filename=log_file,\n",
        "            epoch=epoch+1,\n",
        "            train_loss=train_loss,\n",
        "            train_acc=train_acc,\n",
        "            val_loss=val_loss,\n",
        "            val_acc=val_acc,\n",
        "            epoch_time=epoch_time)\n",
        "\n",
        "        print(f\"[{epoch+1}/{num_epochs}] \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save model\n",
        "    torch.save(seg_model.state_dict(), save_path)\n",
        "    print(f\"Segmentation model saved to {save_path}\")\n",
        "\n",
        "    # Call the common plotting function\n",
        "    plot_training_curves(\n",
        "        num_epochs,\n",
        "        train_acc_history,\n",
        "        val_acc_history,\n",
        "        train_loss_history,\n",
        "        val_loss_history,\n",
        "        lr_per_batch,\n",
        "        prefix=\"segmentation\")\n",
        "\n",
        "\n",
        "    return seg_model\n",
        "\n",
        "def evaluate_segmentation_with_accuracy(seg_model, seg_val_loader, criterion, device):\n",
        "    \"\"\"Compute val_loss and pixel accuracy for binary segmentation in training loop.\"\"\"\n",
        "    seg_model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct_px   = 0\n",
        "    total_px     = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in seg_val_loader:\n",
        "            images = images.to(device)\n",
        "            masks  = masks.long().to(device)\n",
        "\n",
        "            outputs = seg_model(images)['out']\n",
        "            loss = criterion(outputs, masks)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            correct_px += (preds == masks).sum().item()\n",
        "            total_px   += masks.numel()\n",
        "\n",
        "    val_loss = running_loss / len(seg_val_loader.dataset)\n",
        "    val_acc  = correct_px / total_px\n",
        "    return val_loss, val_acc"
      ],
      "metadata": {
        "id": "G8oiktDVT9-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Evaluation of Segmentation Model - Metrics\n",
        "\n",
        "Computing precision, recall, pixel_accuracy, dice, iou\n",
        "\n",
        "\n",
        "*   evaluate_segmentation_metrics() - compute metrics on test dataset (images, trimap GT) by converting trimaps to binary (by ignoring the boundary).\n",
        "*   evaluate_seg_val_loader() - compute metrics on validation set (images, binary pseudo maps)\n",
        "\n"
      ],
      "metadata": {
        "id": "XK4IExm1aFU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OxfordPetsSegmentation(Dataset):\n",
        "    def __init__(self, root, split='test', transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root: Directory for the Oxford-IIIT Pet data.\n",
        "            split: 'trainval' or 'test' in most TorchVision versions (loads ~3680 or ~3669 images).\n",
        "            transform_image: torchvision or custom transforms for the input image.\n",
        "            transform_mask: torchvision or custom transforms for the target mask.\n",
        "        \"\"\"\n",
        "        # Load the base dataset with only segmentation masks\n",
        "        self.dataset = OxfordIIITPet(\n",
        "            root=root,\n",
        "            download=True,\n",
        "            target_types=\"segmentation\",\n",
        "            split=split\n",
        "        )\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # OxfordIIITPet returns: (PIL Image, PIL Mask)\n",
        "        image_pil, mask_pil = self.dataset[idx]\n",
        "\n",
        "        # Convert PIL -> NumPy\n",
        "        image = np.array(image_pil)            # shape [H, W, 3], dtype=uint8\n",
        "        mask  = np.array(mask_pil, dtype=np.uint8)  # shape [H, W], each pixel in {1,2,3} or {0,1,2} depending on dataset\n",
        "\n",
        "\n",
        "        if self.transform:\n",
        "            # Albumentations expects 'image' and 'mask' in a dictionary\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image = augmented[\"image\"]\n",
        "            mask = augmented[\"mask\"]\n",
        "\n",
        "        # The trimap is typically 1=pet, 2=border, 3=background\n",
        "        # Shift to 0-based: (0=pet, 1=border, 2=background)\n",
        "        # If your dataset uses different labeling, adjust here.\n",
        "        mask = mask - 1\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "def evaluate_segmentation_metrics(model, device):\n",
        "\n",
        "  seg_test_transform = A.Compose([\n",
        "      # 1) Resize - to have a consistent baseline size\n",
        "      A.Resize(224, 224),\n",
        "\n",
        "      # 2) Normalize the image (typical ImageNet stats)\n",
        "      A.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                  std=(0.229, 0.224, 0.225)),\n",
        "\n",
        "      # 3) Convert both image and mask to torch tensors\n",
        "      ToTensorV2()\n",
        "  ],  additional_targets={\"mask\": \"mask\"})\n",
        "\n",
        "\n",
        "  test_dataset = OxfordPetsSegmentation(\n",
        "      root=\"./oxford_iiit_data\",\n",
        "      split=\"test\",\n",
        "      transform=seg_test_transform,\n",
        "  )\n",
        "  print(\"Test size:\", len(test_dataset))\n",
        "  test_loader  = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
        "  \"\"\"\n",
        "  Evaluates a binary segmentation model on the test set where\n",
        "  ground truth masks are trimaps (0=BG, 1=FG, 2=Boundary).\n",
        "\n",
        "  Ignores boundary pixels for scoring, merges them out of the confusion matrix.\n",
        "\n",
        "  Returns a dict of {precision, recall, accuracy, dice, iou}.\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "\n",
        "  # Running totals for confusion matrix\n",
        "  total_tp = 0\n",
        "  total_fp = 0\n",
        "  total_tn = 0\n",
        "  total_fn = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for images, trimaps in tqdm(test_loader, total=len(test_loader)):\n",
        "          images  = images.to(device)\n",
        "          # forward pass\n",
        "          outputs = model(images)[\"out\"]  # shape [B, 2, H, W] if binary\n",
        "          preds   = torch.argmax(outputs, dim=1)  # shape [B, H, W], in {0,1}\n",
        "\n",
        "          preds   = preds.cpu().numpy()\n",
        "          trimaps = trimaps.numpy()\n",
        "\n",
        "          # For each sample in the batch\n",
        "          for b in range(images.size(0)):\n",
        "              pred_mask = preds[b]       # shape [H, W], {0,1}\n",
        "              gt_trimap = trimaps[b]     # shape [H, W], {0,1,2}\n",
        "\n",
        "              # Create a mask ignoring boundary pixels (where gt_trimap==2)\n",
        "              ignore_mask = (gt_trimap == 2)\n",
        "\n",
        "              # Flatten\n",
        "              pred_flat = pred_mask.flatten()        # {0,1}\n",
        "              gt_flat   = gt_trimap.flatten()        # {0,1,2}\n",
        "              ignore_flat = ignore_mask.flatten()    # bool\n",
        "\n",
        "              # Filter out boundary indices\n",
        "              valid_idx = np.where(~ignore_flat)[0]  # indices not boundary\n",
        "              if valid_idx.size == 0:\n",
        "                  # entire image might be boundary or something unusual\n",
        "                  continue\n",
        "\n",
        "              pred_valid = pred_flat[valid_idx]  # {0,1}\n",
        "              gt_valid   = gt_flat[valid_idx]    # {0,1}\n",
        "\n",
        "              # Now compute confusion matrix for these pixels\n",
        "              tp = np.sum((pred_valid == 1) & (gt_valid == 1))\n",
        "              fp = np.sum((pred_valid == 1) & (gt_valid == 0))\n",
        "              tn = np.sum((pred_valid == 0) & (gt_valid == 0))\n",
        "              fn = np.sum((pred_valid == 0) & (gt_valid == 1))\n",
        "\n",
        "              total_tp += tp\n",
        "              total_fp += fp\n",
        "              total_tn += tn\n",
        "              total_fn += fn\n",
        "\n",
        "  # Compute metrics\n",
        "  eps = 1e-7  # for safe division\n",
        "  precision = total_tp / (total_tp + total_fp + eps)\n",
        "  recall    = total_tp / (total_tp + total_fn + eps)  # sensitivity\n",
        "  accuracy  = (total_tp + total_tn) / (total_tp + total_tn + total_fp + total_fn + eps)\n",
        "  dice      = 2.0 * total_tp / (2.0 * total_tp + total_fp + total_fn + eps)\n",
        "  iou       = total_tp / (total_tp + total_fp + total_fn + eps)\n",
        "\n",
        "\n",
        "  # Now visualize a few random samples\n",
        "  visualize_test_samples(\n",
        "        model=seg_model,\n",
        "        device=device,\n",
        "        dataset=test_dataset,  # same dataset used in the test_loader\n",
        "        num_samples=5\n",
        "    )\n",
        "\n",
        "  return {\n",
        "      \"precision\": precision,\n",
        "      \"recall\": recall,\n",
        "      \"accuracy\": accuracy,\n",
        "      \"dice\": dice,\n",
        "      \"iou\": iou}"
      ],
      "metadata": {
        "id": "Xi7hEI8ahrYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_seg_val_loader(\n",
        "    model,\n",
        "    val_loader,   # yields (images, binary_masks)\n",
        "    device=\"cuda\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluates a binary segmentation model on a val_loader with pseudo binary GT masks in {0,1}.\n",
        "\n",
        "    Computes the following metrics:\n",
        "      1) Precision\n",
        "      2) Recall (Sensitivity)\n",
        "      3) Accuracy\n",
        "      4) Dice\n",
        "      5) IoU\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    total_tp = 0\n",
        "    total_fp = 0\n",
        "    total_tn = 0\n",
        "    total_fn = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, gt_masks in tqdm(val_loader, total=len(val_loader)):\n",
        "            images = images.to(device)\n",
        "            # forward\n",
        "            outputs = model(images)['out']  # shape [B, 2, H, W] if binary\n",
        "            preds = torch.argmax(outputs, dim=1)  # [B, H, W], in {0,1}\n",
        "\n",
        "            preds = preds.cpu().numpy()      # shape [B,H,W]\n",
        "            gts   = gt_masks.numpy()         # shape [B,H,W], in {0,1}\n",
        "\n",
        "            # For each sample in the batch\n",
        "            for b in range(images.size(0)):\n",
        "                pred_mask = preds[b].flatten()  # [H*W]\n",
        "                gt_mask   = gts[b].flatten()    # [H*W]\n",
        "\n",
        "                tp = np.sum((pred_mask == 1) & (gt_mask == 1))\n",
        "                fp = np.sum((pred_mask == 1) & (gt_mask == 0))\n",
        "                tn = np.sum((pred_mask == 0) & (gt_mask == 0))\n",
        "                fn = np.sum((pred_mask == 0) & (gt_mask == 1))\n",
        "\n",
        "                total_tp += tp\n",
        "                total_fp += fp\n",
        "                total_tn += tn\n",
        "                total_fn += fn\n",
        "\n",
        "    # compute metrics\n",
        "    eps = 1e-7\n",
        "    precision = total_tp / (total_tp + total_fp + eps)\n",
        "    recall    = total_tp / (total_tp + total_fn + eps)\n",
        "    accuracy  = (total_tp + total_tn) / (total_tp + total_fp + total_tn + total_fn + eps)\n",
        "    dice      = 2.0 * total_tp / (2.0 * total_tp + total_fp + total_fn + eps)\n",
        "    iou       = total_tp / (total_tp + total_fp + total_fn + eps)\n",
        "\n",
        "    metrics_dict = {\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"accuracy\": accuracy,\n",
        "        \"dice\": dice,\n",
        "        \"iou\": iou\n",
        "    }\n",
        "\n",
        "    print(\"Validation (Binary) Metrics:\")\n",
        "    for k,v in metrics_dict.items():\n",
        "        print(f\"  {k}: {v:.4f}\")\n",
        "\n",
        "    return metrics_dict\n"
      ],
      "metadata": {
        "id": "suinXlvxOKnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_on_test():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    seg_model = get_deeplab_v3(num_classes=2, pretrained_weights=False)\n",
        "    seg_model.load_state_dict(torch.load(\"weakly_segmentation.pth\", map_location=device), strict=False)\n",
        "    seg_model = seg_model.to(device)\n",
        "    metrics = evaluate_segmentation_metrics(seg_model, device)\n",
        "\n",
        "    print(\"Evaluation on Test Set (Ignoring Boundary Pixels):\")\n",
        "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
        "    print(f\"  Recall:    {metrics['recall']:.4f}\")\n",
        "    print(f\"  Accuracy:  {metrics['accuracy']:.4f}\")\n",
        "    print(f\"  Dice:      {metrics['dice']:.4f}\")\n",
        "    print(f\"  IoU:       {metrics['iou']:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "pcokLKXrSU4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "LSDgLVM7T1PB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # 1. Load your train/val data for classifier\n",
        "    train_loader, val_loader = load_classifier_dataset()\n",
        "\n",
        "    # 2. Create a classifier and train it\n",
        "    classifier = get_resnet50_classifier_model(num_classes=37,pretrained_weights=True)\n",
        "    classifier = train_classifier_with_metrics(\n",
        "        model=classifier,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        num_epochs=20, base_lr=1e-3, max_lr=1e-2, weight_decay=1e-4, clip_grad_norm=None, save_path=\"classifier.pth\", log_file = \"classifier_metrics_log.txt\")\n",
        "\n",
        "    # # Loading Classifier\n",
        "    # classifier = get_resnet50_classifier_model(num_classes=37,pretrained_weights=False)\n",
        "    # classifier.load_state_dict(torch.load(\"/content/classifier.pth\", map_location=device))\n",
        "    # classifier = classifier.to(device)\n",
        "\n",
        "\n",
        "    # Load trainval dataset with Bounding Boxes\n",
        "    train_loader_bbox, train_img_paths = get_dataloader()\n",
        "\n",
        "    # # 3. Generate raw CAMs\n",
        "    gradcam = GradCAM(classifier, target_layer_name=\"layer4\")  # for ResNet\n",
        "    generate_cams(\n",
        "        model=classifier,\n",
        "        data_loader=train_loader_bbox,   # or a combined trainval loader if you want CAM for all\n",
        "        gradcam=gradcam,\n",
        "        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        apply_bbox=True,\n",
        "        output_dir=\"cams_out\", scales=[0.75, 1.0, 1.25]\n",
        "    )\n",
        "\n",
        "    # 4. Apply ReCAM refinement\n",
        "    refine_cams_with_recam(\"cams_out\", refined_dir=\"cams_refined\",\n",
        "                           threshold=0.2, expansion_factor=1.2)\n",
        "\n",
        "    # refine_cams_with_recam_extended(\"cams_out\", output_dir=\"cams_refined\",\n",
        "    #                        iteration_steps=2, erase_ratio=0.15, threshold=0.2, expansion_factor=1.1)\n",
        "\n",
        "    # 5. Generate pseudo masks\n",
        "    generate_pseudo_masks(refined_cam_dir=\"cams_refined\",\n",
        "                          output_mask_dir=\"pseudo_masks\",\n",
        "                          threshold=0.2)\n",
        "\n",
        "    # 6. Train a segmentation model (DeepLab) with these pseudo masks\n",
        "\n",
        "    # Build a dataset from your images and \"pseudo_masks\"\n",
        "\n",
        "    img_mask_pairs = build_img_mask_pairs(\n",
        "    img_dir=\"oxford_iiit_data/oxford-iiit-pet/images\",\n",
        "    mask_dir=\"pseudo_masks\",\n",
        "    img_paths=train_img_paths) # the same list used in the dataset\n",
        "\n",
        "    seg_trainval_dataset = PseudoSegDataset(img_mask_pairs, transform=seg_transform)\n",
        "\n",
        "    # Split dataset into train and val\n",
        "    seg_train_size = int(0.85 * len(seg_trainval_dataset))\n",
        "    seg_val_size = len(seg_trainval_dataset) - seg_train_size\n",
        "    seg_train_ds, seg_val_ds = random_split(seg_trainval_dataset, [seg_train_size, seg_val_size])\n",
        "\n",
        "    seg_train_loader = DataLoader(seg_train_ds, batch_size=32, shuffle=True, num_workers=2)\n",
        "    seg_val_loader = DataLoader(seg_val_ds, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "\n",
        "    seg_model = get_deeplab_v3(num_classes=2, pretrained_weights=True)\n",
        "    seg_model = train_segmentation_with_metrics(\n",
        "        seg_model, seg_train_loader, seg_val_loader,\n",
        "        num_epochs=20, base_lr=1e-3, max_lr=1e-2, weight_decay=1e-4, save_path=\"weakly_segmentation.pth\", log_file = \"segmentation_metrics_log.txt\")\n",
        "\n",
        "\n",
        "    # 7. Testing / visualization / Final evaluation - compute metrics (pseudo masks compared with predicted masks)\n",
        "    evaluate_seg_val_loader(seg_model,seg_val_loader, device)\n",
        "\n",
        "    # Suppose you want to visualize indices 0, 1, and 2 from the val dataset (Image | Pseudo Mask | Predicted Mask)\n",
        "    visualize_segmentation_val_ds(seg_model=seg_model,seg_val_ds=seg_val_ds,\n",
        "                                          device=device,\n",
        "                                          mean=(0.485, 0.456, 0.406),  # typical ImageNet\n",
        "                                          std=(0.229, 0.224, 0.225),\n",
        "                                          indices_to_show=[2, 5, 8]    # pick any indices you like\n",
        "                                          )\n",
        "\n",
        "    # Evaluate on Test dataset - compute metrics (by converting GT trimaps to binary and compare against predicted mask)\n",
        "    # and visualize some predictions - this function calls -> evaluate_segmentation_metrics()\n",
        "    evaluate_on_test()\n",
        "\n",
        "    print(\"Workflow complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "_p87DUHhT2qb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8800a55-fa79-4c15-8d4a-1c1449d70e78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping missing file: /content/oxford_iiit_data/oxford-iiit-pet/annotations/xmls/Abyssinian_104.xml\n",
            "Skipping missing file: /content/oxford_iiit_data/oxford-iiit-pet/annotations/xmls/Bengal_111.xml\n",
            "Skipping missing file: /content/oxford_iiit_data/oxford-iiit-pet/annotations/xmls/samoyed_10.xml\n",
            "Skipping missing file: /content/oxford_iiit_data/oxford-iiit-pet/annotations/xmls/Bengal_175.xml\n",
            "Skipping missing file: /content/oxford_iiit_data/oxford-iiit-pet/annotations/xmls/Egyptian_Mau_14.xml\n",
            "Skipping missing file: /content/oxford_iiit_data/oxford-iiit-pet/annotations/xmls/Egyptian_Mau_156.xml\n",
            "Skipping missing file: /content/oxford_iiit_data/oxford-iiit-pet/annotations/xmls/Egyptian_Mau_186.xml\n",
            "Skipping missing file: /content/oxford_iiit_data/oxford-iiit-pet/annotations/xmls/Ragdoll_199.xml\n",
            "Skipping missing file: /content/oxford_iiit_data/oxford-iiit-pet/annotations/xmls/saint_bernard_15.xml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CAM generation complete!\n",
            "ReCAM refinement complete!\n",
            "Pseudo-label generation complete!\n",
            "Workflow complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DoDfKK99SIsG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}