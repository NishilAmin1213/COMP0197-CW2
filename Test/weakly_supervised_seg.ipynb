{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db1a241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Enhanced Weakly Supervised Segmentation Pipeline\"\"\"\n",
    "\n",
    "# !pip install torch==2.5.0 torchvision --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.v2 as T\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "from torchvision.ops import box_convert, box_iou\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from collections import Counter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Constants\n",
    "NUM_CLASSES = 37  # Oxford-IIIT Pet has 37 breeds\n",
    "SEGMENTATION_CLASSES = 2  # Foreground (pet) and background\n",
    "CAM_THRESHOLD = 0.3\n",
    "PSEUDO_MASK_THRESHOLD = 0.5\n",
    "IMAGE_SIZE = 224\n",
    "DEFAULT_LR = 1e-3\n",
    "DEFAULT_WEIGHT_DECAY = 1e-4\n",
    "DEFAULT_CLIP_GRAD_NORM = None\n",
    "\n",
    "# Data Loading and Preparation\n",
    "def prepare_datasets():\n",
    "    \"\"\"Prepare training and validation datasets with transforms.\"\"\"\n",
    "    train_transform = T.Compose([\n",
    "        T.Resize((256, 256)),\n",
    "        T.RandomResizedCrop((IMAGE_SIZE, IMAGE_SIZE), scale=(0.8, 1.0)),\n",
    "        T.RandomHorizontalFlip(p=0.5),\n",
    "        T.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    val_transform = T.Compose([\n",
    "        T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # Load base dataset\n",
    "    base_dataset = OxfordIIITPet(\n",
    "        root=\"./oxford_iiit_data\",\n",
    "        download=True,\n",
    "        target_types=\"category\",\n",
    "        split=\"trainval\",\n",
    "        transform=None,\n",
    "    )\n",
    "\n",
    "    # Split into train and validation\n",
    "    train_size = int(0.85 * len(base_dataset))\n",
    "    val_size = len(base_dataset) - train_size\n",
    "    train_subset, val_subset = random_split(base_dataset, [train_size, val_size])\n",
    "\n",
    "    # Apply transforms\n",
    "    train_ds = TransformDataset(train_subset, train_transform)\n",
    "    val_ds = TransformDataset(val_subset, val_transform)\n",
    "\n",
    "    return train_ds, val_ds\n",
    "\n",
    "class TransformDataset(Dataset):\n",
    "    \"\"\"Wrapper to apply transforms on the fly.\"\"\"\n",
    "    def __init__(self, subset, transform):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = self.subset[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, target\n",
    "\n",
    "# Model Initialization\n",
    "def get_classifier_model():\n",
    "    \"\"\"Initialize and return the classification model.\"\"\"\n",
    "    model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_features, NUM_CLASSES)\n",
    "    return model.to(device)\n",
    "\n",
    "def get_segmentation_model():\n",
    "    \"\"\"Initialize and return the segmentation model.\"\"\"\n",
    "    model = deeplabv3_resnet50(weights=\"DEFAULT\")\n",
    "    model.classifier[4] = nn.Conv2d(256, SEGMENTATION_CLASSES, kernel_size=1)\n",
    "    return model.to(device)\n",
    "\n",
    "# Training Functions\n",
    "def train_classifier(model, train_loader, val_loader,\n",
    "                   num_epochs=10, lr=DEFAULT_LR,\n",
    "                   weight_decay=DEFAULT_WEIGHT_DECAY, \n",
    "                   clip_grad_norm=DEFAULT_CLIP_GRAD_NORM,\n",
    "                   save_path=\"classifier.pth\"):\n",
    "    \"\"\"Enhanced classifier training function with gradient clipping.\"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            if clip_grad_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_grad_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        val_loss, val_acc = evaluate_classifier(model, val_loader, criterion)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Classifier model saved to {save_path}\")\n",
    "    return model\n",
    "\n",
    "def evaluate_classifier(model, loader, criterion=None):\n",
    "    \"\"\"Enhanced evaluation with both loss and accuracy.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    if criterion is None:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    return running_loss / len(loader), correct / total\n",
    "\n",
    "# CAM Generation and Processing\n",
    "class GradCAM:\n",
    "    \"\"\"Enhanced Grad-CAM implementation with bounding box support.\"\"\"\n",
    "    def __init__(self, model, target_layer_name=\"layer4\"):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.target_layer = None\n",
    "        \n",
    "        # Find target layer\n",
    "        for name, module in self.model.named_children():\n",
    "            if name == target_layer_name:\n",
    "                self.target_layer = module\n",
    "                break\n",
    "        \n",
    "        if self.target_layer is None:\n",
    "            raise ValueError(f\"Layer {target_layer_name} not found\")\n",
    "        \n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        \n",
    "        # Register hooks\n",
    "        self.forward_hook = self.target_layer.register_forward_hook(self._forward_hook)\n",
    "        self.backward_hook = self.target_layer.register_backward_hook(self._backward_hook)\n",
    "    \n",
    "    def _forward_hook(self, module, input, output):\n",
    "        self.activations = output\n",
    "    \n",
    "    def _backward_hook(self, module, grad_in, grad_out):\n",
    "        self.gradients = grad_out[0]\n",
    "    \n",
    "    def __call__(self, x, class_idx=None):\n",
    "        logits = self.model(x)\n",
    "        if class_idx is None:\n",
    "            class_idx = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        # Create one-hot encoding for backprop\n",
    "        one_hot = torch.zeros_like(logits)\n",
    "        for i in range(logits.size(0)):\n",
    "            one_hot[i, class_idx[i]] = 1.0\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        logits.backward(gradient=one_hot, retain_graph=True)\n",
    "        \n",
    "        # Compute CAM\n",
    "        weights = torch.mean(self.gradients, dim=(2, 3), keepdim=True)\n",
    "        cam = (weights * self.activations).sum(dim=1, keepdim=True)\n",
    "        cam = F.relu(cam)\n",
    "        \n",
    "        # Normalize\n",
    "        cam = cam - cam.view(cam.size(0), -1).min(dim=1)[0].view(cam.size(0),1,1,1)\n",
    "        cam = cam / (cam.view(cam.size(0), -1).max(dim=1)[0].view(cam.size(0),1,1,1) + 1e-8)\n",
    "        \n",
    "        return cam\n",
    "\n",
    "def generate_and_refine_cams(model, data_loader, output_dir=\"cams\"):\n",
    "    \"\"\"Generate and refine CAMs for the dataset.\"\"\"\n",
    "    gradcam = GradCAM(model)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (images, _) in enumerate(data_loader):\n",
    "            images = images.to(device)\n",
    "            \n",
    "            with torch.enable_grad():\n",
    "                cams = gradcam(images)\n",
    "            \n",
    "            # Upsample and save\n",
    "            cams = F.interpolate(cams, size=(IMAGE_SIZE, IMAGE_SIZE), \n",
    "                               mode='bilinear', align_corners=False)\n",
    "            cams = cams.squeeze(1).cpu().numpy()\n",
    "            \n",
    "            for b in range(cams.shape[0]):\n",
    "                cam = cams[b]\n",
    "                # Apply ReCAM refinement\n",
    "                cam = recam_refinement(cam)\n",
    "                np.save(os.path.join(output_dir, f\"cam_{i*data_loader.batch_size+b}.npy\"), cam)\n",
    "    \n",
    "    print(\"CAM generation and refinement complete!\")\n",
    "    return output_dir\n",
    "\n",
    "def recam_refinement(cam, expansion_factor=1.2, threshold=CAM_THRESHOLD):\n",
    "    \"\"\"Refine CAMs using ReCAM approach.\"\"\"\n",
    "    mask = (cam >= threshold).astype(np.uint8)\n",
    "    coverage = mask.sum() / (cam.shape[0]*cam.shape[1])\n",
    "    \n",
    "    if coverage < 0.1:\n",
    "        cam = cam * expansion_factor\n",
    "        cam = np.clip(cam, 0, 1)\n",
    "    \n",
    "    return cam\n",
    "\n",
    "# Pseudo Mask Generation\n",
    "def generate_pseudo_masks(cam_dir, output_dir=\"pseudo_masks\"):\n",
    "    \"\"\"Generate binary pseudo masks from refined CAMs.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    cam_files = [f for f in os.listdir(cam_dir) if f.endswith('.npy')]\n",
    "    for cam_file in cam_files:\n",
    "        cam = np.load(os.path.join(cam_dir, cam_file))\n",
    "        mask = (cam >= PSEUDO_MASK_THRESHOLD).astype(np.uint8)\n",
    "        \n",
    "        # Apply morphological refinement\n",
    "        mask = morphological_refinement(mask)\n",
    "        cv2.imwrite(os.path.join(output_dir, cam_file.replace('.npy', '.png')), mask*255)\n",
    "    \n",
    "    print(\"Pseudo mask generation complete!\")\n",
    "    return output_dir\n",
    "\n",
    "def morphological_refinement(mask, kernel_size=3):\n",
    "    \"\"\"Apply morphological operations to refine masks.\"\"\"\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    return cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "# Segmentation Training\n",
    "class PseudoSegDataset(Dataset):\n",
    "    \"\"\"Dataset for segmentation training with pseudo masks.\"\"\"\n",
    "    def __init__(self, image_dataset, mask_dir):\n",
    "        self.image_dataset = image_dataset\n",
    "        self.mask_dir = mask_dir\n",
    "        self.mask_files = sorted([f for f in os.listdir(mask_dir) if f.endswith('.png')])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, _ = self.image_dataset[idx]  # Original image and label\n",
    "        mask_path = os.path.join(self.mask_dir, self.mask_files[idx])\n",
    "        \n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        mask = (mask > 127).astype(np.uint8)  # Ensure binary\n",
    "        \n",
    "        # Convert to tensor and remove channel dimension if present\n",
    "        mask = torch.from_numpy(mask).long()  # Should be (H,W)\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "def train_segmentation(model, train_loader, val_loader, num_epochs=10):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=DEFAULT_LR, weight_decay=DEFAULT_WEIGHT_DECAY)\n",
    "    scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, masks in train_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            # Debug shapes\n",
    "            print(f\"Input shape: {images.shape}, Mask shape: {masks.shape}\")\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)['out']\n",
    "            \n",
    "            # Ensure mask is long type and correct shape\n",
    "            if len(masks.shape) == 4:  # If (B,C,H,W)\n",
    "                masks = masks.squeeze(1)  # Remove channel dim\n",
    "            masks = masks.long()  # Ensure correct dtype\n",
    "            \n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        val_loss = evaluate_segmentation(model, val_loader, criterion)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_segmentation(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            if len(masks.shape) == 4:\n",
    "                masks = masks.squeeze(1)\n",
    "                \n",
    "            outputs = model(images)['out']\n",
    "            loss = criterion(outputs, masks)\n",
    "            running_loss += loss.item()\n",
    "    \n",
    "    return running_loss / len(loader)\n",
    "\n",
    "# Visualization\n",
    "def visualize_results(model, dataset, num_samples=3):\n",
    "    \"\"\"Visualize segmentation results.\"\"\"\n",
    "    model.eval()\n",
    "    indices = random.sample(range(len(dataset)), num_samples)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5*num_samples))\n",
    "    for i, idx in enumerate(indices):\n",
    "        image, mask = dataset[idx]\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred = model(image)['out']\n",
    "            pred_mask = torch.argmax(pred, dim=1).squeeze(0).cpu().numpy()\n",
    "        \n",
    "        image_np = image.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "        image_np = (image_np - image_np.min()) / (image_np.max() - image_np.min())\n",
    "        \n",
    "        plt.subplot(num_samples, 3, i*3+1)\n",
    "        plt.imshow(image_np)\n",
    "        plt.title(\"Input Image\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(num_samples, 3, i*3+2)\n",
    "        plt.imshow(mask, cmap='jet')\n",
    "        plt.title(\"Pseudo Mask\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(num_samples, 3, i*3+3)\n",
    "        plt.imshow(pred_mask, cmap='jet')\n",
    "        plt.title(\"Prediction\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main Workflow\n",
    "def main():\n",
    "    # 1. Prepare data\n",
    "    train_ds, val_ds = prepare_datasets()\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=2)\n",
    "    \n",
    "    # 2. Train classifier\n",
    "    print(\"Training classifier...\")\n",
    "    classifier = get_classifier_model()\n",
    "    classifier = train_classifier(\n",
    "        classifier, train_loader, val_loader,\n",
    "        num_epochs=10, lr=DEFAULT_LR,\n",
    "        weight_decay=DEFAULT_WEIGHT_DECAY,\n",
    "        clip_grad_norm=DEFAULT_CLIP_GRAD_NORM,\n",
    "        save_path=\"classifier.pth\"\n",
    "    )\n",
    "    \n",
    "    # 3. Generate and refine CAMs\n",
    "    print(\"\\nGenerating CAMs...\")\n",
    "    cam_dir = generate_and_refine_cams(classifier, train_loader)\n",
    "    \n",
    "    # 4. Generate pseudo masks\n",
    "    print(\"\\nGenerating pseudo masks...\")\n",
    "    mask_dir = generate_pseudo_masks(cam_dir)\n",
    "    \n",
    "    # 5. Prepare segmentation dataset\n",
    "    seg_train_ds = PseudoSegDataset(train_ds, mask_dir)\n",
    "    seg_train_loader = DataLoader(seg_train_ds, batch_size=8, shuffle=True, num_workers=2)\n",
    "    \n",
    "    # 6. Train segmentation model\n",
    "    print(\"\\nTraining segmentation model...\")\n",
    "    seg_model = get_segmentation_model()\n",
    "    seg_model = train_segmentation(seg_model, seg_train_loader, val_loader)\n",
    "    torch.save(seg_model.state_dict(), \"seg_model.pth\")\n",
    "    \n",
    "    # 7. Visualize results\n",
    "    print(\"\\nVisualizing results...\")\n",
    "    visualize_results(seg_model, seg_train_ds)\n",
    "    \n",
    "    print(\"\\nWorkflow complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
